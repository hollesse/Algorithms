\chapter{Graph Theory}
In this chapter we are going to discuss three problems from \blue{graph theory}.
\begin{enumerate}
\item We present an algorithm to solve the 
      \href{https://en.wikipedia.org/wiki/Disjoint-set_data_structure}{\emph{union-find problem}}.
      In this problem, we are given a set $M$ and a relation $R \subseteq M \times M$.  Our task is
      then to find the equivalence relation that is \blue{generated} by $R$.  The equivalence relation
      generated by the relation $R$ is the smallest equivalence relation $\approx_R$ such that $R \subseteq \approx_R$.
      
      Essentially, the union-find problem is a mathematical problem.  Nevertheless, we will see that 
      it has an important practical application in computer science. 
\item The next problem we solve is the problem to compute the
      \href{https://en.wikipedia.org/wiki/Minimum_spanning_tree}{\emph{minimum spanning tree}}
      of a graph.  Given a weighted graph, this problem asks to find the smallest 
      \href{https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms#subgraph}{subgraph} that 
      connects all vertices of the graph.  We discuss
      \href{https://en.wikipedia.org/wiki/Kruskal%27s_algorithm}{Kruskal's algorithm} 
      for solving this problem.  
\item Finally, we discuss the problem of finding a shortest path in a 
      \href{https://en.wikipedia.org/wiki/Directed_graph}{\emph{weighted directed graph}}.
      We present \href{https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm}{Dijkstra's algorithm} to solve
      this problem.   
\end{enumerate}

\section{The Union-Find Problem}
Assume that we are given a set $M$ together with a relation $R \subseteq M \times M$.  The relation
$R$ is not yet an  equivalence relation on $M$, but this relation \blue{generates} an equivalence relation
$\approx_R$ on $M$.  This \blue{generated equivalence relation} is defined inductively.
\begin{enumerate}
\item For every pair $\pair(x,y) \in R$ we have that $\pair(x, y) \in\; \approx_R$.

      This is the base case of the inductive definition.  It ensures that the relation
      $\approx_R$ is an \blue{extension} of the relation $R$, i.e.~it ensures that $R \subseteq\, \approx_R$.
\item For every $x \in M$ we have $\pair(x,x) \in\; \approx_R$.

      This ensures that the relation $\approx_R$ is \blue{reflexive} on $M$.
\item If $\pair(x,y) \in \approx_R$, then $\pair(y,x) \in\; \approx_R$.

      This  ensures that the relation $\approx_R$ is \blue{symmetric}.
\item If $\pair(x,y) \in\; \approx_R$ and $\pair(y,z) \in\; \approx_R$, then $\pair(x,z) \in\; \approx_R$.

      This clause ensures that the relation $\approx_R$ is \blue{transitive}.
\end{enumerate}
Given this inductive definition, it can be shown that:
\begin{enumerate}
\item $\approx_R$ is an equivalence relation on $M$.
\item If $Q$ is an equivalence relation on $M$ such that $R \subseteq Q$, then $\approx_R \subseteq Q$.
\end{enumerate}
Therefore, the relation $\approx_R$ is the smallest
equivalence relation on $M$ that extends $R$.  In our lesson on linear
algebra we had defined the transitive closure $R^+$ of a binary relation $R$ in a similar way.  In
that lecture, we had then shown that $R^+$ is indeed the smallest transitive relation that extends
$R$.  This proof can easily be adapted to prove the claim given above.

It turns out that a direct implementation of the inductive definition of $\approx_R$ given above is
not very efficient.  Instead, we remind ourselves that there is are one-to-one correspondence
between the equivalence relations $R \subseteq M \times M$ and the
\href{https://en.wikipedia.org/wiki/Partition_of_a_set}{partitions} of $M$.  A set 
$\mathcal{P} \subseteq 2^M$ is a \blue{partition} of $M$ iff the following holds:
\begin{enumerate}
\item $\{\} \not\in \mathcal{P}$,
\item $A \in \mathcal{P} \wedge B \in \mathcal{P} \rightarrow A = B \vee A \cap B = \{\}$,
\item $\ds\bigcup \mathcal{P} = M$.
\end{enumerate}
Therefore, a partition $\mathcal{P}$ of $M$ is a subset of the power set of $M$ such that
every element of $M$ is a member of \magenta{exactly one} set of $\mathcal{P}$ and, furthermore, $\mathcal{P}$ must not contain the
empty set.  We have already seen in the lecture on Linear Algebra that an equivalence relation 
$\approx \;\subseteq M \times M$ gives rise to
\href{https://en.wikipedia.org/wiki/Equivalence_class}{equivalence classes}, 
where the \blue{equivalence class} generated by $x \in M$ is defined as
\\[0.2cm]
\hspace*{1.3cm}
$[x]_\approx := \{ y \mid \pair(x, y) \in\; \approx \}$.
\\[0.2cm]
It was then shown that the set 
\\[0.2cm]
\hspace*{1.3cm}
$\bigl\{ [x]_\approx \;\big|\; x \in M \bigr\}$
\\[0.2cm]
is a partition of $M$.  It was also shown that every partition $\mathcal{P}$ of a set $M$ gives rise
to an equivalence relation $\approx_\mathcal{P}$ that is defined as follows:
\\[0.2cm]
\hspace*{1.3cm}
$x \approx_\mathcal{P} y \;\Longleftrightarrow\; \exists A \in \mathcal{P}:(x \in A \wedge y \in A)$.
\\[0.2cm]
An example will clarify the idea.  Assume that
\\[0.2cm]
\hspace*{1.3cm}
$M := \{ 1,2,3,4,5,6,7,8,9 \}$.
\\[0.2cm]
Then the set 
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{P} := \bigl\{ \{ 1, 4, 7, 9\}, \{3, 5, 8\}, \{2, 6\} \bigr\}$
\\[0.2cm]
is a partition of $M$ since the three sets involved are disjoint and their union is the set $M$.
According to this partition, the elements $1$, $4$, $7$, and $9$ are all
equivalent to each other.  Similarly, the elements $3$, $5$, and $8$ are equivalent to each other,
and, finally, $2$ and $6$ are equivalent.

It turns out that, given a relation $R$, the most efficient way to compute the generated equivalence
relation $\approx_R$ is to compute the partition corresponding to this equivalence relation.  In
order to present the algorithm, we first sketch the underlying idea using a simple example.  Assume
the set $M$ is defined as
\\
\hspace*{1.3cm}
$M := \{ 1,2,3,4,5,6,7,8,9 \}$
\\[0.2cm]
and that the relation $R$ is given as follows:
\\[0.2cm]
\hspace*{1.3cm}
$R := \bigl\{ \pair(1,4), \pair(7,9), \pair(3,5), \pair(2,6), \pair(5,8), \pair(1,9), \pair(4,7) \bigr\}$.
\\[0.2cm]
Our goal is to compute a partition $\mathcal{P}$ of $M$ such that the formula
\\[0.2cm]
\hspace*{1.3cm}
$\pair(x, y) \in R \rightarrow \exists A \in \mathcal{P}:\bigl(x \in A \wedge y \in A)$
\\[0.2cm]
holds.  In order to achieve this goal, we define a sequence of partitions $\mathcal{P}_1$,
$\mathcal{P}_2$, $\cdots$, $\mathcal{P}_n$ such that $\mathcal{P}_n$ achieves our goal.
\begin{enumerate}
\item We start by defining
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_1 := \bigl\{ \{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\}, \{7\}, \{8\}, \{9\} \bigr\}$.
      \\[0.2cm]
      This is clearly a partition of $M$, but it is the trivial one since it generates an equivalence
      relation $\approx$ where we have  $x \approx y$ if and only if $x = y$.  
\item Next, we have to ensure to incorporate our given relation $R$ into this partition.  Since $\pair(1,4) \in R$
      we replace the singleton sets $\{1\}$ and $\{4\}$ by their union.  This leads to the following
      definition of the partition $\mathcal{P}_2$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_2 := \bigl\{ \{1, 4\}, \{2\}, \{3\}, \{5\}, \{6\}, \{7\}, \{8\}, \{9\} \bigr\}$.
\item Since $\pair(7,9) \in R$, we replace the sets $\{7\}$ and $\{9\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_3 := \bigl\{ \{1, 4\}, \{2\}, \{3\}, \{5\}, \{6\}, \{7, 9\}, \{8\} \bigr\}$.
\item Since $\pair(3,5) \in R$, we replace the sets $\{3\}$ and $\{5\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_4 := \bigl\{ \{1, 4\}, \{2\}, \{3,5\}, \{6\}, \{7, 9\}, \{8\} \bigr\}$.
\item Since $\pair(2,6) \in R$, we replace the sets $\{2\}$ and $\{6\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_5 := \bigl\{ \{1, 4\}, \{2,6\}, \{3,5\}, \{7, 9\}, \{8\} \bigr\}$.
\item Since $\pair(5,8) \in R$, we replace the sets $\{3,5\}$ and $\{8\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_6 := \bigl\{ \{1, 4\}, \{2,6\}, \{3,5,8\}, \{7, 9\} \bigr\}$
\item Since $\pair(1,9) \in R$, we replace the sets $\{1,4\}$ and $\{7,9\}$ by their union and define
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{P}_7 := \bigl\{ \{1, 4, 7, 9\}, \{2,6\}, \{3,5,8\} \bigr\}$
\item Next, we have $\pair(4,7) \in R$.  However, $4$ and $7$ are already in the same set.
      Therefore we do not have to change the partition $\mathcal{P}_7$ in this step.
      Furthermore, we have now processed all the pairs in the given relation $R$.
      Therefore, $\mathcal{P}_7$ is the partition that represents the equivalence relation $\approx$ generated
      by $R$.  According to this partition, we have found that
      \\[0.2cm]
      \hspace*{1.3cm}
      $1 \approx 4 \approx 7 \approx 9$, \quad $2 \approx 6$,  \quad and \quad $3 \approx 5 \approx 8$.
\end{enumerate}
 

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    unionFind := procedure(M, R) {
        P := { { x } : x in M };  // start with the trivial partition
        for ([x, y] in R) {
            Sx := find(x, P);
            Sy := find(y, P);
            if (Sx != Sy) {
                P  -= { Sx, Sy };  // remove old sets
                P  += { Sx + Sy }; // add their union
            }
        }
        return P;
    };
    find := procedure(x, P) {
        return arb({ S : S in P | x in S });
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A naive implementation of the union-find algorithm.}
\label{fig:union-find-naive.stlx}
\end{figure}

What we have sketched in the previous example is known as the \blue{union-find algorithm}.
Figure \ref{fig:union-find-naive.stlx} shows a naive implementation of this algorithm.  The
procedure $\texttt{unionFind}$ takes two arguments: $\texttt{M}$ is a set and $\texttt{R}$ is a relation
on $\texttt{M}$.  The purpose of $\texttt{unionFind}$ is to compute the equivalence relation $\approx_\texttt{R}$
that is generated by $\texttt{R}$ on $M$.  This equivalence relation is represented as a partition of $\texttt{M}$.
\begin{enumerate}
\item In line 2 we initialize $\texttt{P}$ as the trivial partition that contains only singleton
      sets.  Obviously, this is a partition of $\texttt{M}$ but it does not yet take the
      relation $\texttt{R}$ into account.
\item The \texttt{for}-loop in line 4 iterates over all pairs $[\texttt{x},\texttt{y}]$ from $\texttt{R}$.
      First, we compute the set $\texttt{Sx}$ that contains $\texttt{x}$ and the set $\texttt{Sy}$ that
      contains $\texttt{y}$.  If these sets are not the same, then $\texttt{x}$ and $\texttt{y}$ are not
      yet equivalent with respect to the partition $\texttt{p}$.  Therefore, the equivalence classes
      $\texttt{Sx}$ and $\texttt{Sy}$ are joined and their union is added to the partition 
      $\texttt{P}$ in line 8, while the equivalence classes $\texttt{Sx}$ and $\texttt{Sy}$ are
      removed from $\texttt{P}$ in line 7.
\item The function $\texttt{find}$ takes an element $\texttt{x}$ of a set $\texttt{M}$ and a partition
      $\texttt{P}$ of $\texttt{M}$.  Since $\texttt{P}$ is a partition of $\texttt{M}$ there must be exactly
      one set $\texttt{S}$ in $\texttt{P}$ such that $\texttt{x}$ is an element of $\texttt{S}$.  This set
      $\texttt{S}$ is then returned.
\end{enumerate}

\subsection{A Tree-Based Implementation}
The implementation shown in Figure \ref{fig:union-find-naive.stlx} is not very efficient.  The
problem is the computation of the union
\\
\hspace*{1.3cm}
$\texttt{Sx} + \texttt{Sy}$.
\\[0.2cm]
If the sets $\texttt{Sx}$ and $\texttt{Sy}$ are represented as binary trees and, for the sake of the
argument, the set $\texttt{Sx}$ contains at most as many elements as the set $\texttt{Sy}$, then the
computational complexity of this operation is
\\
\hspace*{1.3cm}
$\Oh\bigl(\texttt{\#Sx} \cdot \log_2(\texttt{\#Sy})\bigr)$.  
\\[0.2cm]
The reason is that every element of $\texttt{Sx}$ has to be inserted into $\texttt{Sy}$ and this
insertion has a complexity of $\Oh\bigl(\log_2(\#\texttt{Sy})\bigr)$.  Here the expression
$\texttt{\#Sx}$ denotes the size of the set $\texttt{Sx}$ and similarly the expression
$\texttt{\#Sy}$ denotes the size of the set $\texttt{Sy}$.  A more efficient way to
represent these sets is via \blue{parent pointers}:  The idea is that every set is represented as a
tree.  However, this tree is not a binary tree but is rather represented by pointers that
point from a node to its parent.  The node at the root of the tree points to itself.  Then, taking the
union of two sets $\texttt{Sx}$ and $\texttt{Sy}$ is straightforward:  If $\texttt{rx}$ is the node at the root of
the tree representing $\texttt{Sx}$ and $\texttt{ry}$ is the node at the root of the tree representing
$\texttt{Sy}$, then we can just change the parent pointer of $\texttt{ry}$ to point to $\texttt{rx}$.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    find := procedure(x, Parent) {
        if (Parent[x] == x) {
            return x;
        }
        return find(Parent[x], Parent);
    };
    unionFind := procedure(M, R) {
        Parent := { [x, x] : x in M };  
        for ([x, y] in R) {
            rootX := find(x, Parent);
            rootY := find(y, Parent);
            if (rootX != rootY) {
                Parent[rootY] := rootX;  // create union
            }
        }
        Roots := { x : x in M | Parent[x] == x };
        return { { y : y in M | find(y, Parent) == r } : r in Roots };
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A tree-based implementation of the union-find algorithm.}
\label{fig:union-find-tree.stlx}
\end{figure}

Figure \ref{fig:union-find-tree.stlx} on page \pageref{fig:union-find-tree.stlx} shows an
implementation of this idea.  In this implementation, the parent pointers are represented using the
binary relation $\texttt{Parent}$.  
\begin{enumerate}
\item The function $\texttt{find}$ takes a node $\texttt{x}$ and the binary relation $\texttt{Parent}$ 
      representing the parent pointers.  The purpose of the call $\texttt{find}(\texttt{x}, \texttt{Parent})$ is to
      return the root of the tree containing $\texttt{x}$.

      If $\texttt{x}$ is its own parent, then $\texttt{x}$ is already at the root of a tree and therefore 
      we can return $\texttt{x}$ itself in line 3.

      Otherwise, we compute the parent of $\texttt{x}$ and then recursively compute the root of the tree
      containing this parent.  
\item The function $\texttt{unionFind}$ takes a set $\texttt{M}$ and a relation $\texttt{R}$.  It returns
      a partition of $\texttt{M}$ that represents the equivalence relation generated by $\texttt{R}$ on
      $\texttt{M}$.

      The binary relation\footnote{
        In a language like \texttt{C} we would instead use pointers.  Of course, this would be more efficient.
      } $\texttt{Parent}$ is initialized in line 8 so that every node
      points to itself.   This corresponds to the fact that the sets in the initial partition are all
      singleton sets.  

      Next, the function $\texttt{unionFind}$ iterates over all pairs $[\texttt{x}, \texttt{y}]$ from the binary
      relation $\texttt{R}$.  In line 10 and 11 we compute the roots of the trees containing $\texttt{x}$ and
      $\texttt{y}$.  If these roots are identical, then $\texttt{x}$ and $\texttt{y}$ are already
      equivalent and there is nothing to do.  However, if $\texttt{x}$ and $\texttt{y}$ are located 
      in different trees, then these trees need to be merged.  To this end, the parent pointer of
      the root of the tree containing $\texttt{y}$ is changed so that it 
      points to the root of the tree containing $\texttt{x}$.  Therefore, instead of iterating over all
      elements of the set containing $\texttt{y}$, we just change a single pointer.

      Line 16 computes the set of all nodes that are at the root of some tree.  Then, for every root
      $\texttt{R}$ of a tree, line 17 computes the set of nodes corresponding to this tree.
\end{enumerate}

\subsection{Controlling the Growth of the Trees}
As it stands, the algorithm shown in the previous section has a complexity that is $\Oh(n^2)$ in the
worst case where $n$ is the number of elements in the set $ \texttt{M}$.  The worst case happens if there
is just one equivalence class and the tree representing this class degenerates into a list.
Fortunately, it is easy to fix this problem if we keep track of the \magenta{height} of the
different trees.  Then, if we want to join the trees rooted at $\texttt{parentX}$ and
$\texttt{parentY}$, we have a choice: We can either set the parent of the node $\texttt{parentX}$ to
be $\texttt{parentY}$ or we can set the parent of the node $\texttt{parentY}$ to be $\texttt{parentX}$.
If the tree rooted at $\texttt{parentX}$ is smaller than the tree rooted at $\texttt{parentY}$, then we should
use the assignment
\\[0.2cm]
\hspace*{1.3cm}
\texttt{parent[parentX] := parentY;}
\\[0.2cm]
otherwise we should use
\\[0.2cm]
\hspace*{1.3cm}
\texttt{parent[parentY] := parentX;}
\\[0.2cm]
In order to be able to distinguish these case, we store the height of the tree rooted at node
$\texttt{n}$ in the relation $\texttt{Height}$, i.e.~if $\texttt{n}$ is a node, then $\texttt{Height}[n]$ is
the height of the tree rooted at node $\texttt{n}$.  This yields the implementation shown in Figure
\ref{fig:union-find.stlx} on page \pageref{fig:union-find.stlx}.  Provided the size  of the relation
$\texttt{R}$ is bounded by the size $n$ of the set $ \texttt{M}$, the complexity of this
implementation is $\Oh\bigl(n \cdot \log(n)\bigr)$.  However, this excludes the last two lines of
the program.  In practice, the implementation of the function $\texttt{unionFind}$ would omit these
two lines and, instead, return the relation $\texttt{Parent}$ since this is all that is needed to
determine whether two elements $x$ and $y$ are equivalent. 


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    unionFind := procedure(M, R) {
        Parent := { [x, x] : x in M };
        Height := { [x, 1] : x in M };
        for ([x, y] in R) {
            rootX := find(x, Parent);
            rootY := find(y, Parent);
            if (rootX != rootY) {
                if (Height[rootX] < Height[rootY]) {
                    Parent[rootX] := rootY;  
                } else if (Height[rootX] > Height[rootY]) {
                    Parent[rootY] := rootX;  
                } else {
                    Parent[rootY] := rootX;  
                    Height[rootX] += 1;
                }
            }
        }
        Roots := { x : x in M | Parent[x] == x };
        return { { y : y in M | find(y, Parent) == r } : r in Roots };
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A more efficient version of the union-find algorithm.}
\label{fig:union-find.stlx}
\end{figure}

\exercise
We can speed up the implementation previously shown if the set $\texttt{M}$ has the form
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{M} = \{ 1, 2, 3, \cdots, n \}$ \quad where $n \in \mathbb{N}$.
\\[0.2cm]
In this case, the relations $\texttt{Parent}$ and $\texttt{Height}$ can be implemented as arrays.
Develop an implementation that is based on this idea.
\eox

\subsection{Packaging the  Union-Find Algorithm as a Data Structure}
When we later discus  the minimum spanning tree problem,  we will need the union-find algorithm as an
auxiliary data structure.  To this end we present a class that encapsulates the union-find
algorithm.  This class is shown in Figure \ref{fig:union-find-oo.stlx} on page
\pageref{fig:union-find-oo.stlx}.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    class unionFind(M) {
        mParent := { [x, x] : x in M };
        mHeight := { [x, 1] : x in M };
        
      static {
        union := procedure(x, y) {
            rootX := find(x);
            rootY := find(y);
            if (rootX != rootY) {
                if (mHeight[rootX] < mHeight[rootY]) {
                    this.mParent[rootX] := rootY;  
                } else if (mHeight[rootX] > mHeight[rootY]) {
                    this.mParent[rootY] := rootX;  
                } else {
                    this.mParent[rootY] := rootX;  
                    this.mHeight[rootX] += 1;
                }
            }
        };
        find := procedure(x) {
            p := mParent[x]; 
            if (p == x) {
                return x;
            }
            return find(p);
        };
      }
    }

    partition := procedure(M, R) {
        uf := unionFind(M);
        for ([x, y] in R) {
            uf.union(x, y);
        }
        Roots := { x : x in M | uf.find(x) == x };
        return { { y : y in M | uf.find(y) == r } : r in Roots };
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{The class \texttt{unionFind}.}
\label{fig:union-find-oo.stlx}
\end{figure}

\begin{enumerate}
\item The constructor $\texttt{unionFind}$ receives a set $\texttt{M}$ as arguments.  The class
      $\texttt{unionFind}$ maintains two variables:
      \begin{enumerate}
      \item $\texttt{mParent}$ is the dictionary implementing the pointers that point to the parents
             of each node.  If a node $\texttt{n}$ has no parent, then we have
             \\[0.2cm]
             \hspace*{1.3cm}
             \texttt{mParent[n] = n},
             \\[0.2cm]
             i.e.~the roots of the trees point to themselves.  Initially, all nodes are roots, so
             all parent pointers point to themselves.
      \item $\texttt{mHeight}$ is a dictionary containing the heights of the trees.  If $\texttt{n}$ is
            a node, then
            \\[0.2cm]
            \hspace*{1.3cm}
            \texttt{mHeight[n]}
            \\[0.2cm]
            gives the height of the subtree rooted at $\texttt{n}$. As initially all trees contain but
            a single node, these trees all have height $1$.
      \end{enumerate}
\item The method $\texttt{union}$ takes two nodes $\texttt{x}$ and $\texttt{y}$ and joins the trees that
      contain these nodes.  This is achieved by finding their parents $\texttt{parentX}$ and
      $\texttt{parentY}$.  Then, the root of the smaller of the two trees is redirected to point to
      the root of the bigger tree.
\item The method $\texttt{find}$ takes a node $\texttt{x}$ as its argument and computes the root of the
      tree containing $\texttt{x}$. 
\item The function $\texttt{partition}$ is a client of the class $\texttt{unionFind}$.  It takes a set
      $\texttt{M}$ and a relation $\texttt{R}$ on $\texttt{M}$ and computes a partition that corresponds
      to the equivalence relation generated by $\texttt{R}$ on $\texttt{M}$. 
      \begin{enumerate}
      \item First, the function constructs a union-find object $\texttt{uf}$ for the set $\texttt{M}$.
      \item Then the method iterates over all pairs $[\texttt{x},\texttt{y}]$ in the relation $\texttt{R}$ and
            joins the equivalence classes corresponding to $\texttt{x}$ and $\texttt{y}$.
      \item Next, the method collects all nodes $\texttt{x}$ that are at the root of a tree.
      \item Finally, for every root $\texttt{R}$ the method collects those nodes $\texttt{x}$ that are
            part of the tree rooted at $\texttt{R}$.
      \end{enumerate}
\end{enumerate}


\section{Minimum Spanning Trees}
Imagine a telecommunication company that intends to supply internet access to a developing country.
The capital of the country is located at the coast line and is already connected to the internet via
a submarine cable. It is the company's task to connect all of the towns and villages to the capital.
Since most parts of the country are covered by jungle, it is cheapest to build the power lines
alongside existing roads.  Mathematically, this kind of problem can be formulated as the problem of
constructing a \href{https://en.wikipedia.org/wiki/Minimum_spanning_tree}{minimum spanning tree} for
a given \href{https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)#Weighted_graph}{weighted}
\href{https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)#Undirected_graph}{undirected graph}.
Next, we provide the definitions of those notions that are needed to formulate the minimum spanning
tree problem precisely.  Then, we present
\href{https://en.wikipedia.org/wiki/Kruskal%27s_algorithm}{Kruskal's algorithm} for solving the
minimum spanning tree problem. 

\subsection{Basic Definitions}
\begin{Definition}[Weighted Graph] A \blue{weighted undirected graph} is a triple 
   $\langle \nodes, \edges, \weight{\cdot} \rangle$ such that
  \begin{enumerate}
  \item $\nodes$ is the set is a set of  \blue{nodes}.
  \item $\edges$ is the set of  \blue{edges}.  An edge $e$ has the form
        \\[0.2cm]
        \hspace*{1.3cm}
        $\{x, y\}$
        \\[0.2cm]
        and connects $x$ and $y$.  Since $\{x,y\}$ is a set, we have
        \\[0.2cm]
        \hspace*{1.3cm}
        $\{x,y\} \in \edges$ \quad if and only if $\{y,x\} \in \edges$.
        \\[0.2cm]
        Hence, if $x$ is connected to $y$ then $y$ is also connected to $x$.
  \item $\weight{\cdot}: \edges \rightarrow \N$ is a function assigning a \blue{weight} to every edge.

        In practical applications, the weight of an edge is often interpreted as the \blue{cost} or the
        \blue{length} of the edge.
        \conclude
  \end{enumerate}
\end{Definition}

\noindent
A \blue{path} $P$ is a list of the form 
\\[0.2cm]
\hspace*{1.3cm} 
$P = [ x_1, x_2, x_3, \cdots, x_n ]$ 
\\[0.2cm]
such that we have : \\[0.2cm]
\hspace*{1.3cm}
$\{x_i,x_{i+1}\} \in \edges$  \quad for all $i = 1, \cdots, n-1$.
\\[0.2cm]
The set of all paths is denoted as $\mathbb{P}$, i.e.~we define
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathbb{P}  := \bigl\{ P \mid \mbox{$P$ is a path} \bigr\}$.
\\[0.2cm]
A path $P = [ x_1, \cdots, x_n]$ \blue{connects} the nodes $x_1$ and $x_n$.  The \blue{weight} of a path is defined as
the sum of the weights of all of its edges.  
\\[0.2cm]
\hspace*{1.3cm}
 $\ds\Weight{[x_1,x_2, \cdots, x_n]} \df \sum\limits_{i=1}^{n-1} \Weight{\{x_i,x_{i+1}\}}$. 
\\[0.2cm]
A graph is \blue{connected} if for every $x,y \in \nodes$ there is a path connecting $x$ and $y$, i.e.~we have
\\[0.2cm]
\hspace*{1.3cm}
$\forall x, y \in \mathbb{V}: \exists P \in \mathbb{P}: \bigl(P[1] = x \wedge P[\#P] = y\bigr)$.
\\[0.2cm]
A set of edges can be interpreted as graph since the set of nodes can be computed from the edges as
follows: 
\\[0.2cm]
\hspace*{1.3cm}
$\ds\nodes = \bigcup \bigl\{\{x,y\} \,\big|\, \{x,y\} \in \edges \bigr\}$.
\\[0.2cm]
Then, a set of edges $\edges$ is called a \blue{tree} if and only if
\begin{enumerate}
\item the corresponding graph is connected \quad \textbf{and}
\item removing any edge from $\edges$ would result in a graph that is no longer connected.
\end{enumerate}
The \blue{weight} of a tree is the sum of the weights of its edges.

\exercise
Assume that the graph $\langle \nodes, \edges, \weight{\cdot}\rangle$ is a tree.  Prove that the equation
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{card}(\edges) = \textsl{card}(\nodes) - 1$
\\[0.2cm]
holds.  
\vspace*{0.2cm}

\hint
The easiest way to prove this is by induction on $n = \textsl{card}(\mathbb{V})$.  You will need to prove the
following auxiliary claim first: In a tree there is at least one node in $\nodes$ that has
only one neighbouring node. 
\eox

\subsection{Kruskal's Algorithm}
In 1956 the mathematician \href{https://en.wikipedia.org/wiki/Joseph_Kruskal}{Joseph Bernard Kruskal} (1928 -- 2010) 
found a very elegant algorithm for solving the minimum spanning tree problem.   This algorithm makes use
of the \blue{union-find algorithm} that we have shown in the previous section.  The basic idea is as
follows.
\begin{enumerate}
\item In the first step, we create a union-find data structure that contains \blue{singleton trees}
      for all nodes in the graph.  Here, a singleton tree is a tree containing just a single node.
\item Next, we iterate over all edges $\pair(x, y)$ in the graph in \blue{increasing order of their weight}.
      If the nodes $x$ and $y$ are not yet connected, we join their respective equivalence classes by adding
      the edge $\pair(x, y)$ to the tree we are building.

      The fact that we investigate the edges in increasing order of their weight implies that we always choose
      the \blue{cheapest} edge to connect two nodes that are not yet connected.  Hence, Kruskal's algorithm is a
      \href{https://en.wikipedia.org/wiki/Greedy_algorithm}{greedy algorithm}.
\item We stop when the number of edges is 1 less than the number of nodes since, according to the
      previous exercise, the tree must then connect all the nodes of the graph. 
\end{enumerate}
The fact that we iterate over the edges in increasing order of their weight guarantees that the
resulting tree has a minimal weight.
The algorithm is shown in Figure \ref{fig:kruskal.stlx} on page \pageref{fig:kruskal.stlx}.  We
discuss this program next. 

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    mst := procedure(V, E) {
        uf     := unionFind(V);
        Result := {};
        for([w, [x, y]] in E) {
            rx := uf.find(x);
            ry := uf.find(y);
            if (rx != ry) {
                Result += { [w, [x, y]] };
                uf.union(rx, ry);
                if (#Result == #V - 1) {
                    return Result;
                }
            }
        }        
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Kruskal's algorithm.}
\label{fig:kruskal.stlx}
\end{figure}

\begin{enumerate}
\item The main function is the function $\texttt{mst}$, which is short for \underline{m}inimum
      \underline{s}panning \underline{t}ree.  This function takes two arguments:
      $\texttt{V}$ is the set of nodes and $\texttt{E}$ is the set of edges of a given graph.  The edges are represented as
      triples of the form
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{[w, [x, y]]}.
      \\[0.2cm] 
      Here, $\texttt{x}$ and $\texttt{y}$ are the nodes connected by the edge and $\texttt{w}$ is the
      weight of the edge.  The function $\texttt{mst}$ computes a minimum spanning tree of  the
      graph  $\pair(\texttt{V},\texttt{E})$.  It is assumed that this graph is connected.
\item The function $\texttt{mst}$ first creates the union-find data structure $\texttt{uf}$ in line 2.
      Initially, in $\texttt{uf}$ every node is in an equivalence class all by itself, i.e.~nothing is
      yet connected.
\item The spanning tree constructed by the algorithm is stored in the variable $\texttt{Result}$.
      The spanning tree is represented by a set of edges.  
\item Next, we iterate over the edges in $\texttt{E}$.  Since the first part of each edge is its
  weight, the fact that in \textsc{SetlX} sets are stored as \blue{ordered} binary trees ensures that
      we start with the edge that has the smallest weight.
\item For every edge $[\texttt{x},\texttt{y}]$ we check whether $\texttt{x}$ and $\texttt{y}$ are already
      connected.  This is the case if both $\texttt{x}$ and $\texttt{y}$ are in the same tree.
\item If $\texttt{x}$ and $\texttt{y}$ are not connected, the corresponding edge is added to the
      spanning tree and the trees containing $\texttt{x}$ and $\texttt{y}$ are connected by calling the function
      \texttt{union}.
\item The algorithm returns if the tree \texttt{Result} has $\texttt{\#V}-1$ edges, since we know from the previous
      exercise that in that case all nodes have to be connected.
\end{enumerate}


\section{Shortest Paths Algorithms}
In this section we will show two algorithms that solve the
\href{https://en.wikipedia.org/wiki/Shortest_path_problem}{\blue{shortest path problem}}, the 
\href{https://en.wikipedia.org/wiki/Bellman-Ford_algorithm}{Bellman-Ford algorithm} and
\href{https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm}{Dijkstra's algorithm}.  
In order to explain the shortest path problem and the algorithms to solve it, we introduce the notion
of a \href{https://en.wikipedia.org/wiki/Directed_graph}{\blue{weighted directed graph}}.


\begin{Definition}[Weighted Digraph] \
  A \blue{weighted directed graph} (a.k.a.~a \blue{weighted digraph}) is a triple 
  $\langle \nodes, \edges, \weight{\cdot} \rangle$ such that
  \begin{enumerate}
  \item $\nodes$ is the set of \blue{nodes} (sometimes the nodes are known as \blue{vertices}).
  \item $\edges \subseteq \nodes \times \nodes$ is the set of \blue{edges}.
  \item $\weight{\cdot}: \edges \rightarrow \N$ is a function that assigns a positive \blue{length} 
        to every edge.  This length is also known as the \blue{weight} of the edge.
        \eox
  \end{enumerate}
\end{Definition}

\remark
The main difference between a graph and a digraph is that in a digraph the edges are 
\mbox{p\hspace{-0.15cm}\underline{\hspace{0.15cm}airs}} of two
nodes while in a graph the edges are \underline{sets} of two nodes.  Informally, the edges in a
digraph can be viewed as one-way streets, while in a graph they represent streets that can be used in both
directions.  Hence, if a digraph has an edge $\pair(a,b)$, then this edge enables us to get from $a$ to $b$ but
this edge does not enable us to go from $b$ to $a$.  On the other hand, if a graph has an edge $\{a,b\}$, then
this edge enables us to go from $a$ to $b$ as well as to go from $b$ to $a$.
\eox

\begin{Definition}[Path, Path Length]
 A \blue{path} $P$ in a digraph is a list of the form 
\\[0.2cm]
\hspace*{1.3cm} 
$P = [ x_1, x_2, x_3, \cdots, x_n ]$ 
\\[0.2cm]
such that
\\[0.2cm]
\hspace*{1.3cm} $\pair(x_i,x_{i+1}) \in \edges$ \quad holds for all $i = 1, \cdots, n-1$. 
\\[0.2cm]
The set of all paths is denoted as $\paths$.
The length of a path is the sum of the length of all edges:
\\[0.2cm]
\hspace*{1.3cm}
$\ds\Weight{[x_1,x_2, \cdots, x_n]} \df \sum\limits_{i=1}^{n-1} \Weight{\pair(x_i,x_{i+1})}$. 
\\[0.2cm]
If  $p = [x_1, x_2, \cdots, x_n]$ is a path then  $p$ \blue{connects} the node $x_1$ with the node
$x_n$.  We denote the set of all paths that connect the node $v$ with the node $w$ as
\\[0.2cm]
\hspace*{1.3cm} 
 $\paths(v,w) \df \bigl\{ [x_1, x_2, \cdots, x_n] \in \paths \bigm| x_1 = v \,\wedge\, x_n = w \bigr\}$.
\end{Definition}

\noindent
Now we are ready to state the shortest path problem.

\begin{Definition}[Shortest Path Problem] \lb
  Assume a weighted digraph  
  $G = \langle \nodes, \edges, \weight{\cdot} \rangle$ 
  and a node $\source \in \nodes$ is given.  Then the \blue{shortest path problem} asks to compute
  the following function:
  \\[0.2cm]
  \hspace*{1.3cm} $\spath: \nodes \rightarrow \N$ \\[0.1cm]
  \hspace*{1.3cm} $\spath(v) \df \texttt{min}\bigl\{ \weight{p} \bigm| p \in \paths(\source,v) \bigr\}$.
  \\[0.2cm]
  Furthermore, given that $\spath(v) = n$, we would like to be able to compute a path 
  $p \in \paths(\source,v)$ such that $\weight{p} = n$.
  \eox
\end{Definition}

\subsection{The Bellman-Ford Algorithm}
The first algorithm we discuss is the
\href{https://en.wikipedia.org/wiki/Bellman-Ford_algorithm}{\blue{Bellman-Ford algorithm}}.
It is named after the mathematicians 
\href{https://en.wikipedia.org/wiki/Richard_E._Bellman}{Richard E.~Bellman} \cite{bellman:58} and 
\href{https://en.wikipedia.org/wiki/L._R._Ford_Jr.}{Lester R.~Ford Jr.} \cite{ford:56} who found this algorithm
independently and published their results in 1958 and 1956, respectively.  Figure
\ref{fig:moore.stlx} on page \pageref{fig:moore.stlx} shows the implementation of a variant of this 
algorithm in \textsc{SetlX}.  This variant of the algorithm was suggested by 
\href{https://en.wikipedia.org/wiki/Edward_F._Moore}{Edward F.~Moore} \cite{moore:59}.


\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    shortestPath := procedure(source, Edges) {
        Distance := { [source, 0] };
        Fringe   := { source };
        while (Fringe != {}) {
            u := from(Fringe);
            for ([v,l] in Edges[u]) {
                if (Distance[v] == om || Distance[u]+l < Distance[v]) {
                    Distance[v] := Distance[u] + l;
                    Fringe      += { v };
                }
            }
        }
        return Distance;
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{The Bellman-Ford algorithm to solve the shortest path problem.}
  \label{fig:moore.stlx}
\end{figure} 

\noindent
\begin{enumerate}
\item The function $\texttt{shortestPath}(\texttt{source}, \texttt{Edges})$ is called with two arguments:
      \begin{enumerate}
      \item $\texttt{source}$ is the start node.  Our intention is to compute the distance of all
            vertices from the node $\texttt{source}$.
      \item $\texttt{Edges}$ is a \blue{functional binary relation} that encodes the set of edges of the graph.  For
            every node $x$ the set $\texttt{Edges}[x]$ has the form
            \\[0.2cm]
            \hspace*{1.3cm}
            $\bigl\{ [y_1, l_1], \cdots, [y_n, l_n] \bigr\}$.
            \\[0.2cm]
            This set is interpreted as follows: For every $i = 1,\cdots,n$ there is an edge
            $\langle x, y_i \rangle$ pointing from $x$ to $y_i$ and this edge has the length $l_i$.

            For example, the relation $\texttt{Edges}$ defined below defines a simple digraph.
            In that digraph, there is an edge from node $\texttt{"a"}$ to node $\texttt{"b"}$ which has a
            length of $2$ and there is another edge from node $\texttt{"a"}$ to the node $\texttt{"c"}$ that has a length of $3$.
            \begin{verbatim}
        Edges := { ["a", {["b", 2], ["c", 3]}], 
                   ["b", {["d", 1]} ],
                   ["c", {["e", 3]} ],  
                   ["d", {["e", 2], ["f", 4]} ],  
                   ["e", {["f", 1]} ],
                   ["f", {} ]
                 };
           \end{verbatim}
      \end{enumerate}
\item The variable $\texttt{Distance}$ is a \blue{functional binary relation}.  When the computation is
      successful, for every node $x$ this relation will contain a pair of the form
      $[x, \texttt{sp}(x)]$ showing that the node $x$ has distance $\texttt{sp}(x)$ from the node $\texttt{source}$.

      The node $\texttt{source}$ has distance $0$ from the node $\texttt{source}$ and initially this is
      all we know.  Hence, the relation $\texttt{Distance}$ is initialized as the set $\bigl\{[\texttt{source},0]\bigr\}$.
\item The variable $\texttt{Fringe}$ is a set of those nodes where we already have an estimate of
      their distance from the  node \texttt{source}, but where we haven't yet computed the distances of their
      neighbours from the node \texttt{source}. 
      Every iteration of the \texttt{while} loop computes the distances of all those nodes $y$ that are
      connected to a node in $\texttt{Fringe}$ by an edge $\pair(x,y)$.
      Initially we only know that the node $\texttt{source}$ is connected to the node \texttt{source}.
      Therefore, we initialize the set $\texttt{Fringe}$ as the set
      \\[0.2cm]
      \hspace*{1.3cm}
      $\{ \texttt{source} \}$.
\item As long as there are nodes left in the set $\texttt{Fringe}$ we pick an arbitrary node
      $\texttt{u}$ from $\texttt{Fringe}$ and remove it from $\texttt{Fringe}$. 
\item Next, we compute the set of all nodes $\texttt{v}$ that can be reached from $\texttt{u}$ and check whether we
      have found a shorter path leading to any of these nodes.  There are two cases.
      \begin{enumerate}
      \item If there is an edge from $\texttt{u}$ to a node $\texttt{v}$ and  $\texttt{Distance}[\texttt{v}]$ is still
            undefined, then we hadn't yet found a path leading to $\texttt{v}$.
      \item Furthermore, there are those nodes $\texttt{v}$ where we had already found a path leading from
            $\texttt{source}$ to $\texttt{v}$ but the length of this path is longer than the length of the path
            that we get when we first visit $\texttt{u}$ and then proceed to $\texttt{v}$ via the edge $\pair(\texttt{u},\texttt{v})$.
      \end{enumerate}
      We compute the distance of the path leading from $\texttt{source}$ to $\texttt{u}$ and then to
      $\texttt{v}$ in both of these cases and add $\texttt{v}$ to the fringe.
\item The algorithm terminates when the set $\texttt{Fringe}$ is empty because in that case we don't
      have any means left to improve our estimation of the distance function.
\end{enumerate}

\subsection{Dijkstra's Algorithm}
\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    shortestPath := procedure(source, Edges) {
        Distance := { [source, 0] };
        Fringe   := { [0, source] };
        Visited  := { source };
        while (Fringe != {}) {
            [d, u]  := first(Fringe);
            Fringe  -= { [d, u] };
            for ([v,l] in Edges[u]) {
                if (Distance[v] == om || d + l < Distance[v]) {
                    Fringe      -= { [Distance[v], v] };
                    Distance[v] := d + l;
                    Fringe      += { [d + l, v] };
                }
            }
            Visited += { u };
        }
        return Distance;
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Dijkstra's algorithm to solve the shortest path problem.}
\label{fig:dijkstra.stlx}
\end{figure}

\noindent
The Bellman-Ford algorithm doesn't specify how the nodes are picked from the set $\texttt{Fringe}$.
The result of this non-determinism is that a node can be picked from $\texttt{Fringe}$ and removed from
$\texttt{Fringe}$ only to be reinserted into $\texttt{Fringe}$ later.  This is inefficient.
In 1959 \href{https://en.wikipedia.org/wiki/Edsger_W._Dijkstra}{Edsger W.~Dijkstra} (1930 -- 2002) \cite{dijkstra:59}
published an algorithm that removed this non-determinism.  Furthermore, Dijkstra's algorithm guarantees that it
is never necessary to reinsert a node back into the set $\texttt{Fringe}$.  Dijkstra had the idea to always choose
the node that has the smallest distance to the node $\texttt{source}$.  To do this, we just have to
implement the set  $\texttt{Fringe}$ as a priority queue where the priority of a node is given by the
distance of this node to the node $\texttt{source}$.  Figure \ref{fig:dijkstra.stlx} on page
\pageref{fig:dijkstra.stlx} shows an implementation of Dijkstra's algorithm in \textsc{SetlX}.

The program shown in Figure \ref{fig:dijkstra.stlx} has an additional variable called $\texttt{Visited}$.
This variable contains the set of those nodes that have been  \blue{visited} by the algorithm.
To be more precise, $\texttt{Visited}$ contains those nodes $\texttt{u}$ that have been removed from the
priority queue $\texttt{Fringe}$ and for which all neighbouring nodes, i.e.~those nodes $y$ such that
there is an edge $\pair(u,y)$, have been examined.
The set $\texttt{Visited}$ isn't used in the implementation of the algorithm since the variable is
only written but is never read.
I have introduced the variable $\texttt{Visited}$ in order to be able to formulate the invariant that
is needed to prove the \blue{correctness} of the algorithm.  This invariant is
\\[0.2cm]
\hspace*{1.3cm}
$\forall \texttt{u}\in\texttt{Visited}: \texttt{Distance}[\texttt{u}] = \texttt{sp}(\texttt{u})$,
\\[0.2cm]
i.e.~for all nodes $\texttt{u} \in \texttt{Visited}$, the functional relation $\texttt{Distance}$
already contains the length of the shortest path leading from $\texttt{source}$ to $x$.  
\vspace*{0.1cm}

\proof 
We prove by induction that every time that a node $\texttt{u}$ is added to the set
$\texttt{Visited}$ we have that
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{Distance}[\texttt{u}] = \texttt{sp}(\texttt{u})$ 
\\[0.2cm]
holds.  To begin, observe that the program has only two lines where the set $\texttt{Visited}$ is changed.
We only need to inspect these lines.

\begin{enumerate}
\item \textbf{Base Case:}  
      Initially, the set $\texttt{Visited}$ contains only the node $\texttt{source}$ and we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{sp}(\texttt{source}) = 0 = \texttt{Distance}[\texttt{source}]$.
      \\[0.2cm]
      Surely, the distance from $\texttt{source}$ to $\texttt{source}$ is $0$.  Hence the claim is true initially.
\item \textbf{Induction Step:}
      In line 15 we add the node $\texttt{u}$ to the set $\texttt{Visited}$.  Immediately before $\texttt{u}$ is
      inserted there are two cases: If $\texttt{u}$ is already a member of the set $\texttt{Visited}$, the
      claim is true by induction hypothesis.  Hence we only need to consider the case where
     $\texttt{u}$ is not a member of the set $\texttt{Visited}$ before it is inserted.

      The proof now proceeds \red{by contradiction} and we \blue{assume} that we have
      \\[0.2cm]
      \hspace*{1.3cm} $\texttt{Distance}[\texttt{u}] > \texttt{sp}(\texttt{u})$
      \\[0.2cm]
      Then there must exist a shortest path 
      \\[0.2cm]
      \hspace*{1.3cm} $p = [ x_0 = \texttt{source}, x_1, \cdots, x_n = \texttt{u} ]$
      \\[0.2cm]
      leading from $\texttt{source}$ to $\texttt{u}$ that has length $\texttt{sp}(u)$ and this length is less
      than $\texttt{Distance}[\texttt{u}]$.  Define  $i\in\{0,\cdots,n-1\}$ to be the index such that
      \\[0.2cm]
      \hspace*{1.3cm}
      $x_0\in \texttt{Visited}$, $\cdots$, $x_i\in \texttt{Visited}$ \quad but \quad $x_{i+1} \not\in \texttt{Visited}$
      \\[0.2cm]
      holds.  Hence $x_i$ is the first node in the path  $p$ such that $x_{i+1}$ is not a member of
      $\texttt{Visited}$.  Such an index $i$ has to exist because $\texttt{u} \not\in \texttt{Visited}$.
      Before the node $x_i$ is added to the set $\texttt{Visited}$, all nodes $y$ that are connected
      to $x_i$ via an edge $\pair(x_i, y)$ are examined and for these nodes the function
      $\texttt{Distance}$ is updated if this edge leads to a shorter path.  In particular, $x_{i+1}$ has
      been examined and $\texttt{Distance}[x_{i+1}]$ has been recomputed.  At the latest, the node
      $x_{i+1}$ has been added to the set  $\texttt{Fringe}$ at this time, although, of course, it
      could have been added already earlier.  Furthermore, we must have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{Distance}[x_{i+1}] = \texttt{sp}(x_{i+1})$,
      \\[0.2cm]
      because by induction hypothesis we have that $\texttt{Distance}[x_i] = \texttt{sp}(x_i)$ and 
      the edge $\pair(x_i,x_{i+1})$ is part of a shortest path from  $x_i$ to $x_{i+1}$.
      
      As we have assumed that $x_{i+1} \not\in \texttt{Visited}$, the node $x_{i+1}$ still has to be an
      element of the priority queue $\texttt{Fringe}$ at this point.  Therefore we must have
      $\texttt{Distance}[x_{i+1}] \geq \texttt{Distance}[u]$, since otherwise  $x_{i+1}$ would have been
      chosen from the priority queue $\texttt{Fringe}$ before $\texttt{u}$ has been chosen, but then $x_{i+1}$
      would be a member of $\texttt{Visited}$.

      Since $\texttt{sp}(x_{i+1}) = \texttt{Distance}[x_{i+1}]$ we now have 
      \\[0.2cm]
      \hspace*{1.3cm} 
      $\texttt{sp(u)} \geq \texttt{sp}(x_{i+1}) = \texttt{Distance}[x_{i+1}] \geq \texttt{Distance[u]} > \texttt{sp(u)}$,
      \\[0.2cm]
      i.e.~we have shown the \red{contradiction}
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{sp(u)} > \texttt{sp(u)}$,
      \\[0.2cm]
      This shows that the \blue{assumption} $\texttt{Distance[u]} > \texttt{sp(u)}$ has to be wrong.  On the
      other hand, since we always have $\texttt{Distance[u]} \geq \texttt{sp(u)}$ we can only conclude that
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{Distance[u]} = \texttt{sp(u)}$
      \\[0.2cm]
      holds for all nodes $\texttt{u} \in \texttt{Visited}$. \qed
\end{enumerate}

\exercise
Improve the implementation of Dijkstra's algorithm given above so that the algorithm also computes
the shortest path for every node that is reachable from $\texttt{source}$.
\eox


\subsection{Complexity}
If a node $\texttt{u}$ is removed from the priority queue $\texttt{Fringe}$, the node is added to the set
$\texttt{Visited}$.  The invariant that was just proven implies that in that case
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{sp(u)} = \texttt{Distance[u]}$
\\[0.2cm]
holds.  This implies that the node $\texttt{u}$ can never be reinserted into the priority queue
$\texttt{Fringe}$, because a node $\texttt{v}$ is only inserted in $\texttt{Fringe}$ if either 
 $\texttt{Distance}[\texttt{v}]$ is still undefined or if  the  value $\texttt{Distance}[\texttt{v}]$ decreases.  
Inserting a node into a priority queue containing  $n$ elements can be bounded by
$\Oh\bigl(\log_2(n)\bigr)$.  As the priority queue never contains more than $\#\nodes$ nodes and we
can insert every node at most once, insertion into the priority queue can be bounded by
\\[0.2cm]
\hspace*{1.3cm}
$\Oh\bigl(\#V \cdot \log_2(\#V)\bigr)$.
\\[0.2cm]
We also have to analyse the complexity of removing a node from the fringe. 
The number of times the assignment
\\[0.2cm]
\hspace*{1.3cm}
\texttt{Fringe -= \{ [dvOld, v] \};} 
\\[0.2cm]
is executed is bounded by the number of edges leading to the node $\texttt{v}$.
Removing an element from a set containing $n$ elements can be bounded by
 $\Oh\bigl(\log_2(n)\bigr)$.  Hence, removal of all nodes from the Fringe can be bounded by
\\[0.2cm]
\hspace*{1.3cm}
$\Oh\bigl(\#\edges \cdot \log_2(\#\nodes)\bigr)$.
\\[0.2cm]
Here,  $\#\edges$ is the number of edges.  Hence the complexity of Dijkstra's algorithm can be
bounded by the expression \\[0.2cm]
\hspace*{1.3cm} $\Oh\bigl((\#\edges + \#\nodes) * \ln(\#\nodes)\bigr)$. \\[0.2cm]
If the number of edges leading  to a given node is bounded by a fixed number, e.g.~if there
are at most 4 edges leading to a given node, then the number of edges is a fixed multiple of the
number of nodes.  In this case, the complexity of 
 Dijkstra's algorithm is given by the expression  
\\[0.2cm]
\hspace*{1.3cm}
$\Oh\bigl(\#\nodes * \log_2(\#\nodes)\bigr)$.

\section{Topological Sorting}
Assume that you have a huge project to manage.  In order to do so, you can use 
\href{https://en.wikipedia.org/wiki/Program_evaluation_and_review_technique}{\textsc{Pert}}, which is short for
\blue{\underline{p}rogram \underline{e}valuation and \underline{r}eview \underline{t}echnique}.  Using \texttt{Pert},
you break your project into a number of \blue{tasks}
\\[0.2cm]
\hspace*{1.3cm}
$T := \{t_1,\cdots,t_n\}$.  
\\[0.2cm]
Furthermore, there are \blue{dependencies} between the task: These dependencies take the form $s \prec t$, where $s$ and
$t$ are tasks.  A dependency of the form $s \prec t$ means that the task $s$ has to be finished before the task $t$
can start.  For example, if the overall project is to get dressed in the morning\footnote{
  In order to keep things manageably simple, I have made the assumption that the person that needs to get dressed is male.
}, the set $T$ of task is given
as follows:
\\[0.2cm]
\hspace*{1.3cm}
$T := \{ \textsl{socks}, \textsl{trousers}, \textsl{shirt}, \textsl{shoes} \}$.
\\[0.2cm]
The elements of $T$ are interpreted as follows:
\begin{itemize}
\item \textsl{socks}: Put on socks.
\item \textsl{trousers}: Put on trousers.
\item \textsl{shirt}: Put on shirt.
\item \textsl{shoes}: Put on shoes.
\end{itemize}
There are some \blue{dependencies} between these tasks:  For example, putting on the shoes first and then trying to
put on the socks does not work.  In detail, we have the following dependencies between the different tasks:
\begin{enumerate}
\item $\textsl{socks} \prec \textsl{shoes}$,
\item $\textsl{trousers} \prec \textsl{shoes}$,
\item $\textsl{shirt} \prec \textsl{trousers}$.
\end{enumerate}
Now the problem is to order the tasks such that the dependencies are satisfied.  For example, for the project
of dressing up in the morning, the following schedule would work:
\\[0.2cm]
\hspace*{1.3cm}
\textsl{socks}, \textsl{shirt}, \textsl{trousers}, \textsl{shoes}.
\\[0.2cm]
For the simple task of dressing up in the morning, most of you would get the scheduling right most of the
time\footnote{If you have a bad hangover, the correct scheduling can turn out to be more challenging.}.
However, complex projects can have ten thousands of tasks and even more dependencies between these tasks.  For
example, the US-lead invasion in the second Irak war was a project consisting of more than $30\,000$ tasks.
The US invasion plan for the \href{https://en.wikipedia.org/wiki/China}{People's Republic of China} even
contain more than 2 million tasks!  Ordering the tasks for a project of this size is very difficult to do by
hand.  Instead, a technique called \blue{topological sorting} is used. 

\subsection{Formal Definition of Topological Sorting}
In order to better grasp the idea of a topological sorting problem, we define it formally as a graph
theoretical problem.
\begin{Definition}[Topological Sorting Problem, Solution of a Topological Sorting Problem]  \hspace*{\fill} \\
  A \href{https://en.wikipedia.org/wiki/Topological_sorting}{topological sorting problem} is a directed graph
  $\pair(T, D)$ where
  \begin{itemize}
  \item $T$ is called the set of \blue{tasks} and 
  \item $D \subseteq T \times T$ is called the set of \blue{dependencies}.

        If $\pair(s, t) \in D$, then this is written as 
        \\[0.2cm]
        \hspace*{1.3cm}
        $s \prec t$, \quad which is read as $t$ \blue{depends} on $s$.
  \end{itemize}
  For conciseness, we abbreviate ``topological sorting problem'' as \textsc{Tsp}.
  
  Mathematically, a \blue{solution} to a topological sorting problem is a \blue{linear ordering} $\leq$ that
  satisfies 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\forall s, t \in T:\bigl(s \prec t \rightarrow s < t\bigr)$,
  \\[0.2cm]
  where $s < t$ is defined in terms of $s \leq t$ as follows
  \\[0.2cm]
  \hspace*{1.3cm}
  $s < t \;\stackrel{\mathrm{def}}{\Longleftrightarrow}\; s \leq t \wedge s \not= t$.
  \\[0.2cm]
  In practical applications, the set of tasks $T$ is always finite.  Then, the linear ordering is computed as a \blue{list}
  $S$ of tasks.  In order for a list $S$ of tasks to be a solution of a \textsc{Tsp} $\pair(T,D)$, we need to have 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\forall i,j \in \{1,\cdots,\#S\}:\bigl(S[i] \prec S[j] \rightarrow i < j\bigr)$,
  \\[0.2cm]
  i.e.~if the task $S[j]$ depends on the task $S[i]$, then $S[i]$ needs to precede $S[j]$ in the list $S$.
  Furthermore, every task $t \in T$ occurs exactly once in $S$. This requirement is equivalent to the equation
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{card}(T) = \texttt{length}(S)$,
  \\[0.2cm]
  i.e.~the number of elements of the set $T$ is the same as the length of the list $S$.
  \eox
\end{Definition}

\subsection{Computing the Solution of a \textsc{Tsp}}
Next, we present \blue{Kahn's algorithm} \cite{kahn:1962} for solving a \textsc{Tsp}.  The basic idea is very
simple. Ask yourself:  How can a list $S$ that is supposed to be a solution to a \textsl{Tsp} $\pair(T, D)$ start?
Of course, it can only start with a task that does not depend on any other task.  Therefore, if we are
unfortunate and every task depends on some other task, there is no way to solve the \textsc{Tsp}.  Otherwise,
we can just pick any task that does not depend on another task to be the first task and remove it from the set
of tasks.  After that it's just rinse and repeat.  Therefore, Kahn's algorithm for solving a \textsc{Tsp}
$\pair(T, D)$ works as follows: 
\begin{enumerate}
\item Initialize the list $\texttt{S}$ of tasks to the empty list.
\item Pick a task $t \in T$ that does not depend on any other task and append this task to $\texttt{S}$.

      If there is no such task, the \textsc{Tsp} is not solvable and the algorithm terminates with failure.
\item Remove the task $t$ from both $T$ and $D$, i.e.~if there is a pair $\pair(t, s) \in T$ for some $s$, then
      this pair is removed from $S$.
\item If $T$ is not yet empty, go back to step 1.  
\end{enumerate}
When the algorithm terminates, the list \texttt{S} is a solution to the \textsc{Tsp} $\pair(T, D)$.

In order to implement this algorithm we need to think about the data structures that are needed.
In order to be able to pick a task $t \in T$ that does not depend on any other task, we need a set of all those
tasks that do not depend on any other task.  Using graph theoretical manner of speaking we call this set the
set of \blue{orphans}, since in graph theory, if we have an edge $\pair(s, t)$, then $s$ is called a parent of
$t$ and, therefore, a node with no parents is called an \blue{orphan}.  In order to maintain the set of
orphans, we need to be able to compute the \blue{parents} of each node.  Furthermore, when we remove a node $t$
from the graph after inserting it into the list \texttt{S}, we have to compute the set of children of that
node.  The reason is that we have to update the set of parents of the children of $t$.  This leads to the algorithm
shown in Figure \ref{fig:topologocal-sorting.stlx} on page \pageref{fig:topologocal-sorting.stlx}.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    topoSort := procedure(T, D) {
        Parents  := { [t, {}] : t in T };  // dictionary of parents
        Children := { [t, {}] : t in T };  // dictionary of children
        for ([s, t] in D) {
            Children[s] += { t }; Parents[t] += { s };
        }
        Orphans := { t : [t, P] in Parents | P == {} };
        Sorted  := [];
        while (T != {}) {
            assert(Orphans != {}, "The graph is cyclic!");
            t := from(Orphans);  T -= { t };  Sorted += [t];
            for (s in Children[t]) {
                Parents[s] -= { t };
                if (Parents[s] == {}) {
                    Orphans += { s };
                }
            }
        }
        return Sorted;
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Kahn's algorithm for topological sorting.}
\label{fig:topologocal-sorting.stlx}
\end{figure}

\begin{enumerate}
\item In order to compute the parents and children of a given node efficiently, Kahn's algorithm
      uses two dictionaries: $\texttt{Parents}$ and $\texttt{Children}$.
      In lines 2 and 3 we initialize these dictionaries to be empty and the \texttt{for}-loop in line 4 fills both of these
      dictionaries: If there is a dependency $\pair(\texttt{s}, \texttt{t})$, then $\texttt{t}$ is a child of
      $\texttt{s}$ and $\texttt{s}$ is a parent of $\texttt{t}$.  Therefore, $\texttt{t}$ is added to the set
      $\texttt{Children[s]}$ and $\texttt{s}$ is added to the set $\texttt{Parents[t]}$.
\item In line 7, we compute the set $\texttt{Orphans}$ of those tasks that have no $\texttt{Parents}$.
\item In line 8, the list $\texttt{Sorted}$ is initialized.  When the algorithm completes without failure, this list will
      contain a solution of the \textsc{Tsp}.
\item As long as there are still tasks in the set $\texttt{T}$ that have not been inserted into the list
      $\texttt{Sorted}$ the algorithm keeps going.
\item If at any point the set of tasks $\texttt{T}$ that need to be scheduled is not yet empty but all remaining
      tasks still have parents, i.e.~depend on the completion of some other task, then the given \textsc{Tsp}
      is not solvable and the algorithm terminates with a failure message.
\item Otherwise, we remove a task $\texttt{t}$ from the set of orphaned tasks, remove this task from $\texttt{T}$
      and append it to the list of scheduled tasks $\texttt{Sorted}$.
\item Finally, we have to update the parent dictionary so that for all children $\texttt{s}$ of the task
      $\texttt{t}$, the task $\texttt{t}$ is no longer a parent of the task $\texttt{s}$.  Furthermore, if this
      implies that the task $\texttt{s}$ has no parents left, it needs to be added to the set $\texttt{Orphans}$.
\end{enumerate}

\subsection{Complexity}
If the number of dependencies that any given task is involved in is bounded by some fixed constant and $n$ is
the number of tasks,  then the complexity of Kahn's algorithm as given in Figure \ref{fig:topologocal-sorting.stlx}
is  
\\[0.2cm]
\hspace*{1.3cm}
$\Oh\bigl(n \cdot \log_2(n)\bigr)$.
\\[0.2cm]
The reason is that building the dictionary \texttt{Parents} as well as the dictionary \texttt{Children} both
have the complexity $\Oh\bigl(n \cdot \log_2(n)\bigr)$. Furthermore, maintaining  the set \texttt{Orphans} has a complexity of
$\Oh\bigl(n \cdot \log_2(n)\bigr)$.




%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "algorithms"
%%% End: 
