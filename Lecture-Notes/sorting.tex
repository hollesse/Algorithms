\chapter{Sorting}
In this chapter, we assume that we have been given a list $L$.  The elements of $L$ are members of
some set $S$.  If we want to \blue{sort} the list $L$ we have to be able to compare these elements
to each other.   Therefore, we assume that $S$ is equipped with a binary relation $\leq$ which is
\blue{reflexive}, \blue{anti-symmetric} and \blue{transitive}, i.~e.~we have  
\begin{enumerate}
\item $\forall x \el S \colon x \leq x$,
\item $\forall x, y \el S \colon \bigl(x \leq y \wedge y \leq x
  \rightarrow x = y\bigr)$, 
\item $\forall x, y, z \el S \colon \bigl(x \leq y \wedge y \leq z \rightarrow x \leq z\bigr)$. 
\end{enumerate}
A pair $\langle S, \leq \rangle$ where $S$ is a set and $\leq \;\subseteq S \times S$ is a relation
on $S$ that is \blue{reflexive}, \blue{anti-symmetric} and \blue{transitive} is called a
\href{http://en.wikipedia.org/wiki/Partially_ordered_set}{partially ordered set}.  
If, furthermore
\\[0.2cm]
\hspace*{1.3cm}
$\forall x, y \el S \colon\bigl( x \leq y \vee y \leq x\bigr)$
\\[0.2cm]
holds, then the pair $\langle S, \leq \rangle$ is called a 
\href{http://en.wikipedia.org/wiki/Totally_ordered_set}{totally ordered set} and
the relation $\leq$ is called a \blue{total order} or a \blue{linear order}.

\examples
\begin{enumerate}
\item $\langle\mathbb{N}, \leq \rangle$ is a totally ordered set.
\item $\langle 2^{\mathbb{N}}, \subseteq \rangle$ is a partially ordered set but it is not a totally
      ordered set.  For example, the sets $\{1\}$ and $\{2\}$ are not comparable since we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\{1\} \not\subseteq \{2\}$ \quad and \quad  $\{2\} \not\subseteq \{1\}$.
\item If $P$ is the set of employees of some company and if we define for given employees
      $a,b \el P$
      \\[0.2cm]
      \hspace*{1.3cm}
      $a \preceq b$ \quad iff \quad  $a$ does not earn more than $b$, 
      \\[0.2cm]
      then the pair $\langle P, \leq \rangle$ is not a partially ordered set.  The reason is that
      the relation $\preceq$ is not anti-symmetric:  If Mr.~Smith earns as much as
      Mrs.~Robinson, then we have both
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{Smith} \preceq \mathrm{Robinson}$ \quad and \quad $\mathrm{Robinson} \preceq \mathrm{Smith}$
      \\[0.2cm]
      but obviously $\mathrm{Smith} \not= \mathrm{Robinson}$.
\end{enumerate}
In the examples given above we see that it does not make much sense to sort subsets of $\mathbb{N}$.
However, we can sort natural numbers with respect to their size and we can also sort employees with
respect to their income.  This shows that, in order to sort,  we do not necessarily need a totally
ordered set.  In order to capture the requirements that are needed to be able to sort we introduce
the notion of a \href{http://en.wikipedia.org/wiki/Preorder}{quasiorder}.
\pagebreak

\begin{Definition}[Quasiorder]  \hspace*{\fill} \\
{\em
  A pair $\langle S, \preceq\rangle$ is a \blue{quasiorder}  if $\preceq$ is a 
  binary relation on $S$ such that we have the following:
  \begin{enumerate}
  \item $\forall x \el S\colon x \preceq x$. \hspace*{\fill} (reflexivity)
  \item $\forall x, y, z \el S \colon \bigl(x \preceq y \wedge y \preceq z \rightarrow x \preceq z\bigr)$. 
         \hspace*{\fill} (transitivity)
  \end{enumerate}
  If, furthermore,
  \\[0.2cm]
  \hspace*{1.3cm}
  $\forall x, y \el S \colon \bigl(x \preceq y \vee y \preceq x\bigr)$ \hspace*{\fill} (linearity)
  \\[0.2cm]
  holds, then $\langle S, \preceq \rangle$ is called a \blue{total quasiorder}.  This will be
  abbreviated as \textsc{TQO}.
}
\end{Definition}
A quasiorder $\langle S, \preceq \rangle$ does not require the relation $\preceq$ to be
anti-symmetric.  Nevertheless, the notion of a quasiorder is very closely related to the notion of a
linear order.  The reason is as follows:  If $\langle S, \preceq \rangle$ is a quasiorder, then we
can define an equivalence relation $\approx$ on $S$ by setting
\\[0.2cm]
\hspace*{1.3cm}
$x \approx y \stackrel{\mbox{\scriptsize def}}{\Longleftrightarrow} x \preceq y \wedge y \preceq x$. 
\\[0.2cm]
If we extend the order $\preceq$ to the equivalence classes generated by the relation $\approx$,
then it can be shown that this extension is a linear order.
\vspace*{0.3cm}

Let us assume that $\langle M, \preceq \rangle$ is a  \textsc{TQO}.  Then the \blue{sorting problem}
is defined as follows:
\begin{enumerate}
\item A list $L$ of elements of $M$ is given.
\item We want to compute a list $S$ such that we have the following: 
  \begin{enumerate}
  \item $S$ is sorted ascendingly: \\[0.2cm]
        \hspace*{1.3cm} 
        $\forall i \el \{ 1, \cdots, \#S-1 \} \colon S[i] \preceq S[i+1]$ 
        \\[0.2cm]
        Here, the length of the list $S$ is denoted as $\#S$ and $S[i]$ is the $i$-th element of $S$.
  \item The elements of $M$ occur in $L$ and $S$ with the same frequency: \\[0.2cm]
        \hspace*{1.3cm} 
        $\forall x\el M \colon \textsl{count}(x,L) = \textsl{count}(x,S)$.
        \\[0.2cm]
        Here, the function $\textsl{count}(x,L)$ returns the number of occurrences of $x$ in $L$.
        Therefore, we have: \\[0.2cm]
        \hspace*{1.3cm}
        $\textsl{count}(x,L) := \# \bigl\{ i \el \{1,\cdots,\#L\} \mid L[i] = x \bigr\}$.
        \\[0.2cm]
        Sometimes, this second requirement is changed as follows:
        \\[0.2cm]
        \hspace*{1.3cm}
        $\forall x \in s: \textsl{count}(x,S) \leq 1 \wedge \forall x\el M \colon \bigl(\textsl{count}(x,L) > 0 \leftrightarrow \textsl{count}(x,S) = 1\bigr)$.
        \\[0.2cm]
        Hence, in this case we require that the sorted list $s$ does not contain duplicate elements.
        Of course, an object $x$ should only occur in $s$ if it also occurs in $L$.  If we change
        the second requirement in this way, then the main purpose of sorting is to remove duplicate
        elements from a list.  This is actually one common application of sorting in practice.  The
        reason this application is so common is the following: A list that contains every element at
        most once can be viewed as representing a set.

        \exercise
        Assume a list $S$ is sorted and contains every object at most once.  Develop an efficient
        algorithm for testing whether a given object $x$ is a member of the list $S$.

        \textbf{Hint}: Try to develop an algorithm that follows the \blue{divide-and-conquer}
        paradigm. 
  \end{enumerate}
\end{enumerate}
Next, we present various algorithms for solving the sorting problem.  We start with two algorithms
that are very easy to implement: \blue{insertion sort} and \blue{selection sort}.  However, the
efficiency of these algorithms is far from optimal.  Next, we present \blue{quick sort} and 
\blue{merge sort}.  Both of these algorithms are very efficient when implemented carefully.
However, the implementation of these algorithms is much more involved.


\section{Insertion Sort \label{chap:insertionSort}}
Let us start our investigation of sorting algorithms with
\href{http://en.wikipedia.org/wiki/Insertion_sort}{insertion sort}.  We will describe the
algorithm via a set of equations.
\begin{enumerate}
\item If the list $L$ that has to be sorted is empty, then the result is the empty list: 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{sort}([]) = []$.
\item Otherwise, the list $L$ must have the form $[x] + R$. Here, $x$ is the first element of $L$
      and $R$ is the rest of $L$, i.~e.~everything of $L$ but the first element.  In order to sort
      $L$ we first sort the rest $R$ and then we insert the element $x$ into the resulting list in a
      way that the resulting list remains sorted:
      \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{sort}\bigl([x] + R\bigr) = \mathtt{insert}\bigl(x, \mathtt{sort}(R)\bigr)$.
\end{enumerate}
Inserting $x$ into an already sorted list $S$ is done according to the following specification:
\begin{enumerate}
\item If $S$ is empty, the result is the list $[x]$: \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{insert}(x,[]) = [x]$.
\item Otherwise, $S$ must have the form $[y] + R$.  In order to know where to insert $x$ we have to
      compare $x$ and $y$.
      \begin{enumerate}
      \item If $x \preceq y$, then we have to insert $x$ at the front of the list $S$: \\[0.2cm]
            \hspace*{1.3cm}
            $x \preceq y \rightarrow \mathtt{insert}\bigl(x, [y] + R\bigr) = [x,y] + R$. 
      \item Otherwise, $x$ has to be inserted recursively into the list $R$: \\[0.2cm]
            \hspace*{1.3cm}
            $\neg x \preceq y \rightarrow \mathtt{insert}\bigl(x, [y] + R\bigr) = [y] + \mathtt{insert}(x,R)$. 
      \end{enumerate}
\end{enumerate}

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.3cm,
                  xrightmargin  = 0.3cm
                ]
    sort := procedure(L) {
        match (L) {
            case []     : return [];
            case [x | R]: return insert(x, sort(R));
        }
    };
    insert := procedure(x, L) {
        match (L) {
            case []                  : return [x];
            case [y | R] |   x <= y  : return [x | L];
            case [y | R] | !(x <= y) : return [y | insert(x, R)];
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Implementing \blue{insertion sort} in \textsc{SetlX}.}
  \label{fig:insertion-sort.stlx}
\end{figure} 

\noindent
Figure \ref{fig:insertion-sort.stlx} shows how the \blue{insertion-sort} algorithm can be implemented 
in \textsc{SetlX}.
\begin{enumerate}
\item The definition of the function \texttt{sort} makes use of the \texttt{match} statement
      that is available in \texttt{SetlX}.  Essentially, the \texttt{match} statement is an upgraded
      \texttt{switch} statement.  Therefore, line 3 is executed if the list $L$ is empty.

      Line 4 tests whether $L$ can be written as  
      \\[0.2cm]
      \hspace*{1.3cm}
      $L = [x | R]$.
      \\[0.2cm]
      Here, $x$ is the first element of $L$ while $R$ contains all elements but the first element of $L$.  
\item The definition of the function $\mathtt{insert}$ also uses a 
      \texttt{match} statement.  However, in the last two cases the match statement
      also has a logical condition attached via the operator ``\texttt{|}'':
      In line 10, this condition checks whether $x \leq y$, while line 11 checks for the
      complementary case.
\end{enumerate}

\subsection{Complexity of Insertion Sort}
We will compute the number of comparisons that are done in the implementation of \texttt{insert}.
Before doing so, let us note that the function \texttt{insert.stlx} can be rewritten as shown in Figure
\ref{fig:insert.stlx}.  In comparison to Figure \ref{fig:insertion-sort.stlx}, we have dropped the
test
\\[0.2cm]
\hspace*{1.3cm}
``\texttt{!(x <= y)}'' 
\\[0.2cm]
from line 5 since it is unnecessary:  If control ever reaches line 5, it must
have skipped line 4 before and for a non-empty list that can only happen if the test 
``\texttt{x <= y}'' fails.
\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm
                ]
    insert := procedure(x, L) {
        match (L) {
            case []               : return [x];
            case [y | R] | x <= y : return [x, y | R];
            case [y | R]          : return [y | insert(x, R)];
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{More efficient implementation of $\mathtt{insert}$.}
  \label{fig:insert.stlx}
\end{figure} 

Let us compute the number of evaluations of the comparison operator ``\texttt{<=}'' in line 4 in the
worst case if we call $\texttt{sort}(L)$ with a list $L$ of length $n$. In order to do that, we have to
compute the number of evaluations of the operator ``\texttt{<=}'' when 
 $\texttt{insert}(x,L)$ is evaluated for a list $L$ of length $n$.  Let us denote this number as 
$a_n$.  The worst case happens if $x$ is bigger than every element of $L$ because in that case the
test ``\texttt{x <= y}'' in line 4 of Figure \ref{fig:insert.stlx} will always evaluate to
\texttt{false} and therefore \texttt{insert} will keep calling itself recursively.
Then we have
\\[0.2cm]
\hspace*{1.3cm}
$a_0 = 0$ \quad and \quad $a_{n+1} = a_n + 1$. 
\\[0.2cm]
A trivial induction shows that this recurrence relation has the solution
\\[0.2cm]
\hspace*{1.3cm} 
$a_n = n$.
\\[0.2cm]
In the worst case the evaluation of $\mathtt{insert}(x,L)$ will lead to $n$ comparisons for a list
$L$ of length $n$.  The reason is simple:  If $x$ is bigger than any element of $L$, then we have to
compare $x$ with every element of $L$ in order to insert $x$ into $L$.

Next, let us compute the number of comparisons that have to be done when calling
$\texttt{sort}(L)$ in the worst case for a list  $L$ of length $n$.  Let us denote this number as
$b_n$. The worst case happens if $L$ is sorted in reverse order, i.~e.~if $L$ is sorted
descendingly.   
Then we have \\[0.2cm]
\hspace*{1.3cm}
 $b_1 = 0$ \quad and \quad $b_{n+1} = b_n + n$, \hspace*{\fill} (1)
\\[0.2cm]
because for a list of the form $L = [x] + R$ of length $n+1$ we first have to sort the list $R$
recursively.  As $R$ has length $n$ this takes $b_n$ comparisons.  After that, the call
$\mathtt{insert}(x, \mathtt{sort(R)})$ 
inserts the element $x$ into $\mathtt{sort}(R)$.  We have previously seen that this takes $n$
comparisons if $x$ is bigger than all elements of $\mathtt{sort}(R)$ and if the list $L$ is sorted
descendingly this will indeed be the case.

If we substitute $n$ by $n-1$ in equation $(1)$ we find
\\[0.2cm]
\hspace*{1.3cm}
$b_n = b_{n-1} + (n - 1)$.
\\[0.2cm]
This recurrence equation is solved by expanding the right hand side successively as follows:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  b_{n} & = & b_{n-1} + (n - 1)                     \\ 
        & = & b_{n-2} + (n - 2) + (n - 1)           \\ 
        & \vdots &                                  \\
        & = & b_{n-k} + (n - k) + \cdots + (n - 1)  \\ 
        & \vdots &                                  \\
        & = & b_{1} + 1 + \cdots + (n - 1)      \\[0.2cm] 
        & = & \ds b_{1} + \sum\limits_{i = 1}^{n - 1} i \\[0.4cm]
        & = & \frac{1}{2} \cdot n \cdot (n - 1),
\end{array}
$
\\[0.2cm]
because $b_1 = 0$ and the sum of all natural numbers from 1 up to  $n - 1$ is given as
\\[0.2cm]
\hspace*{1.3cm}
$\ds\sum\limits_{i = 0}^{n - 1} i  = \frac{1}{2} \cdot n \cdot (n - 1)$.
\\[0.2cm]
This can be shown by a straightforward induction.  Therefore, in the worst case the number $b_n$ of
comparisons needed for sorting a list of length $n$  satisfies 
\\[0.2cm]
\hspace*{1.3cm}
$\ds b_n = \frac{1}{2} \cdot n^2 - \frac{1}{2} \cdot n = \frac{1}{2} \cdot n^2 + \Oh(n)$.
\\[0.2cm]
Therefore, in the worst case the number of comparisons is given as $\Oh(n^2)$ and hence
\blue{insertion sort} is quadratic.


Next, let us consider the best case.  The best case happens if the list $L$ is already sorted
ascendingly.  Then, the call of 
$\mathtt{insert}(x,\mathtt{sort}(R))$ only needs a single comparison.  This time, the recurrence
equation for the number $b_l$ of comparisons when sorting $L$ satisfies
 \\[0.2cm]
\hspace*{1.3cm}
$b_1 = 0$ \quad and \quad $b_{n+1} = b_n + 1$. 
\\[0.2cm]
Obviously, the solution of this recurrence equation is $b_n = n-1$.  Therefore, in the best case
\blue{insertion sort} is linear.  This is as good as it can possibly get because when sorting a list $L$ we
must at least inspect all of the elements of $L$ and therefore we will always have at least a linear
amount of work to do.


\section{Selection Sort}
Next, we discuss 
\href{http://en.wikipedia.org/wiki/Selection_sort}{selection sort}.  In order to sort a given
list $L$ this algorithm works as
follows:
\begin{enumerate}
\item If $L$ is empty, the result is the empty list: \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{sort}([]) = []$.
\item Otherwise, we compute the smallest element of the list $L$ and we remove this element from
      $L$.  Next, the remaining list is sorted recursively.  Finally, the smallest element is added
      to the front of the sorted list:
      \\[0.2cm]
      \hspace*{1.3cm} 
      $L \not= [] \rightarrow \mathtt{sort}\bigl(L\bigr) = \bigl[\texttt{min}(L)\bigr] +  \mathtt{sort}\bigl(\mathtt{delete}(\texttt{min}(L), L)\bigr)$.
\end{enumerate}
The algorithm to delete an element $x$ from a list $L$ is formulated recursively.  There are three cases:
\begin{enumerate}
\item If $L$ is empty, we have \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{delete}(x, []) = []$.
\item If $x$ is equal to the first element of $L$, then the function \texttt{delete} returns the
      rest of $L$: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{delete}(x, [x] + R) = R$.
\item Otherwise, the element $x$ is removed recursively from the rest of the list: \\[0.2cm]
      \hspace*{1.3cm}   
      $x \not = y \rightarrow \mathtt{delete}(x, [y] + R) = [y] + \mathtt{delete}(x,R)$.
\end{enumerate}
Finally, we have to specify the computation of the minimum of a list $L$:
\begin{enumerate}
\item The minimum of the empty list is bigger than any element.  Therefore we have 
      \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{min}([]) = \infty$.
\item In order to compute the minimum of the list $[x] + R$ we compute the minimum of $R$ and
      then use the binary function \texttt{min}: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{min}([x] + R) = \mathtt{min}\bigl(x, \mathtt{min}(R) \bigr)$. 

      Here, the binary function \texttt{min} is defined as follows: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{min}(x,y) = \left\{
      \begin{array}{ll}
        x  & \mbox{if $x \preceq y\,$;} \\
        y  & \mbox{otherwise.} \\
      \end{array}\right.
      $
\end{enumerate}
Figure \ref{fig:selection-sort.setlx} on page \pageref{fig:selection-sort.setlx} shows an
implementation of selection sort in \textsc{SetlX}.  There is no need to implement the function
\texttt{min} as this function is already predefined in \textsc{SetlX}. 
The implementation of $\mathtt{delete}(x,L)$ is \blue{defensive}:  Normally, $\mathtt{delete}(x, L)$
should only be called if $x$ is indeed an element of the list $L$.   Therefore, if the algorithm tries 
to delete an element from the empty list, something must have gone wrong.  The predefined \texttt{assert}
function will provide us with an error message in this case.


\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.3cm,
                  xrightmargin  = 0.3cm
                ]
    sort := procedure(L) {
        if (L == []) {
            return [];
        }
        x := min(L);
        return [x] + sort(delete(x, L));
    };
    delete := procedure(x, L) {
        match (L) {
            case []              : assert(false, "$x$ not in $L$");
            case [y | R] | y == x: return R;
            case [y | R] | y != x: return [y] + delete(x, R);
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Implementing \blue{selection sort} in \textsc{SetlX}.}
  \label{fig:selection-sort.setlx}
\end{figure}
%$

\subsection{Complexity of Selection Sort}
In order to be able to analyse the complexity of \blue{selection sort} we have to count the number
of comparisons that are performed when $\mathtt{min}(L)$ is computed.  We have 
\\[0.2cm]
\hspace*{1.3cm} 
$\mathtt{min}([x_1,x_2,x_3,\cdots,x_n]) = \mathtt{min}(x_1, \mathtt{min}(x_2, \mathtt{min}(x_3, \cdots \mathtt{min}(x_{n-1},x_n) \cdots )))$. 
\\[0.2cm]
Therefore, in order to compute $\texttt{min}(L)$ for a list $L$ of length $n$ the binary function \texttt{min}
is called $(n-1)$ times.  Each of these calls of \texttt{min} causes an evaluation of the comparison
operator ``$\preceq$''.  If the number of evaluations of the comparison operator used to sort a list
$L$ of length $n$ is written as $b_n$, we have \\[0.2cm]
\hspace*{1.3cm}
$b_0 = 0$ \quad und \quad $b_{n+1} = b_n + n$. 
\\[0.2cm]
The reasoning is as follows: In order to sort a list of $n+1$ elements using selection sort we first
have to compute the minimum of this list.  We need $n$ comparisons for this.  Next, the minimum is
removed from the list and the remaining list, which now contains only $n$ elements, is sorted
recursively.  We need $b_n$ evaluations of the comparison operator for this recursive invocation of
\texttt{sort}.

When investigating the complexity of \blue{insertion sort} we had arrived at the same recurrence
relation. We had found the solution of this recurrence relation to be
\\[0.2cm]
\hspace*{1.3cm} $\ds b_n = \frac{1}{2} \cdot n^2 - \frac{1}{2}\cdot n = \frac{1}{2} \cdot n^2 +
\Oh(n)$. 
\\[0.2cm]
It seems that the number of comparisons done by \blue{insertion sort} is the same as the number of
comparisons needed for \blue{selection sort}.  However, let us not jump to conclusions.
The algorithm \blue{insertion sort} needs
$\frac{1}{2}\cdot n \cdot (n-1)$ comparisons only in the \underline{worst} case while \blue{selection sort}
\underline{alwa}y$\!\!$\underline{$\;$s} uses $\frac{1}{2} \cdot n\cdot(n-1)$ comparisons.
In order to compute the minimum of a list of length $n$ we always have to do $n-1$ comparisons.
However, in order to insert an element into a list of $n$ elements, we only expect to do about
$\frac{1}{2} \cdot n$ comparisons on average.  The reason is that we expect about half the elements  to
be less than the element to be inserted.  Hence, we only have to compare the element to be inserted
with half of the remaining elements.  Therefore, the average number of comparisons used by
insertion sort is only
\\[0.2cm]
\hspace*{1.3cm}
 $\ds \frac{1}{4} \cdot n^2 + \Oh(n)$
\\[0.2cm]
and this is half as much as the number of comparisons used by \blue{selection sort}.  Therefore, on
average we expect \blue{selection sort} to need about twice as many comparisons as \blue{insertion sort}.
Furthermore,  in many practical applications of sorting the lists that have to be sorted are already
partially sorted and have only a few elements that are out of place.  In these cases,
\blue{insertion sort} is, in fact,  more efficient than any other sorting algorithm.


\section{\href{http://en.wikipedia.org/wiki/Merge_sort}{Merge Sort}}
Next, we discuss \blue{merge sort}.  This algorithm
is the first \blue{optimally efficient} sorting algorithm that we encounter: We will see that merge sort 
only needs $\Oh\bigl(n \cdot \log_2(n)\bigr)$ comparisons to sort a list of $n$ elements.  Later, we will prove
that every algorithm that needs to compare its elements in order to sort them has at least this complexity.  
The \blue{merge sort} algorithm was discovered by
\href{http://en.wikipedia.org/wiki/John_von_Neumann}{John von Neumann} in 1945.  John von Neumann 
was one of the most prominent mathematicians of the last century.  
% Furthermore, he published a
% number of influential papers in physics.  Finally, he is one of the fathers of the nuclear bomb and
% had been one of the fiercest advocaters of its use.  Hence, he shares the joint guilt for the 
% \href{http://en.wikipedia.org/wiki/Atomic_bombings_of_Hiroshima_and_Nagasaki}{murder} of more than
% $200\,000$ innocent civilian at the end of the second world war.

In order to sort a list $L$ the algorithm proceeds as follows:
\begin{enumerate}
\item If $L$ has less than two elements, then $L$ is already sorted.  Therefore we have: 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\#L < 2 \rightarrow \mathtt{sort}(L) = L$.
\item Otherwise, the list $L$ is split into two lists that have approximately the same size.
      These lists are sorted recursively.  Then, the sorted lists are merged in a way that the
      resulting list is sorted: \\[0.2cm]
      \hspace*{1.3cm} 
      $\#L \geq 2 \rightarrow \mathtt{sort}(L) =
         \mathtt{merge}\bigl(\mathtt{sort}\bigl(\texttt{L[..n\symbol{92}2]}\bigr),
         \mathtt{sort}\bigl(\texttt{L[n\symbol{92}2+1..]}\bigr)\bigr)
     $
     \\[0.2cm]
     Here, $\texttt{L[..\#L\symbol{92}2]}$ is the first part of the list, while
     $\texttt{L[\#L\symbol{92}2+1..]}$ is the second part.  If the length of $L$ is even, both part have the same number of
     elements, otherwise the second part has one element more than the first part.  The function \texttt{merge}
     takes two sorted lists and combines their element in a way that the resulting list is sorted.
\end{enumerate}
Figure \ref{fig:merge-sort.stlx} shows how these equations can be implemented as a \textsc{SetlX}
program.  

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    sort := procedure(L) {
        if (#L < 2) {
            return L;
        }
        n := #L;
        [L1, L2] := [L[.. n\2], L[n\2+1 ..]];
        return merge(sort(L1), sort(L2));
    };
    merge := procedure(L1, L2) {
        match ([L1, L2]) {
            case [      [],       L2] : return L2;
            case [      L1,       []] : return L1;
            case [[x | R1], [y | R2]] : if (x <= y) {
                                            return [x] + merge(R1, L2);
                                        } else {
                                            return [y] + merge(L1, R2);
                                        }
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{The \blue{merge sort} algorithm implemented in \textsc{SetlX}.}
  \label{fig:merge-sort.stlx}
\end{figure}
\begin{enumerate}
\item If the list $L$ has less than two elements, it is already sorted and, therefore, it
      can be returned as it is.
\item If the List $L$ has $n$ elements, then splitting $L$ is achieved by putting the first $n\symbol{92}2$
      elements into the list $L_1$ and the remaining elements into the list $L_2$.
\item These lists are sorted recursively and the resulting sorted lists are then \blue{merged}.
\end{enumerate}
Finally, we need to specify how two sorted lists $L_1$ and $L_2$ are merged in a way that the resulting list
is also sorted.
\begin{enumerate}
\item If the list $L_1$ is empty, the result is $L_2$: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{merge}([], L_2) = L_2$.
\item If the list $L_2$  is empty, the result is $L_1$: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{merge}(L_1, []) = L_1$.
\item Otherwise, $L_1$ must have the form $[x|R_1]$ and $L_2$ has the form $[y|R_2]$.
      Then there is a case distinction with respect to the comparison of $x$ and $y$:
      \begin{enumerate}
      \item $x \preceq y$.

            In this case, we merge $R_1$ and $L_2$ and put $x$ at the beginning of this list:
            \\[0.2cm]
            \hspace*{1.3cm} 
            $x \preceq y \rightarrow \mathtt{merge}\bigl([x|R_1], [y|R_2]\bigr) = \bigl[x\bigr] + \mathtt{merge}\bigl(R_1,[y|R_2]\bigr)$.
      \item $\neg x \preceq y$.

            Now we merge $L_1$ and $R_2$ and put $y$ at the beginning of this list:
            \\[0.2cm]
            \hspace*{1.3cm} 
            $\neg x \preceq y \rightarrow \mathtt{merge}\bigl([x|R_1], [y|R_2]\bigr) = \bigl[y \bigr] + \mathtt{merge}\bigl([x|R_1],R_2\bigr)$.
      \end{enumerate}
\end{enumerate}

\subsection{Complexity of Merge Sort}
Next, we compute the number of comparisons that are needed to sort a list of $n$
elements via merge sort.  To this end, we first analyse the number of comparisons that 
are done in a call of $\mathtt{merge}(L_1, L_2)$.   In order to do this we define the function \\[0.2cm]
\hspace*{1.3cm} 
$\mathtt{cmpCount}: \textsl{List}(M) \times \textsl{List}(M) \rightarrow \mathbb{N}$ 
\\[0.2cm]
such that, given two lists $L_1$ and $L_2$ of elements of some set $M$ the expression $\mathtt{cmpCount}(L_1, L_2)$ returns the
number of comparisons needed to compute $\texttt{merge}(L_1,L_2)$. 
Our claim is that, for any lists $L_1$ and $L_2$ we have  
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{cmpCount}(L_1, L_2) \leq \# L_1 + \# L_2$. 
\\[0.2cm]
The proof is done by induction on $\#L_1 + \#L_2$.
\begin{enumerate}
\item[I.A.:] $\#L_1 + \#L_2=0$.

             Then both $L_1$ and $L_2$ are empty and therefore the evaluation of
             $\mathtt{merge}(L_1, L_2)$ does not need any comparisons.  Therefore, we have 
             \\[0.2cm]
             \hspace*{1.3cm}
             $\mathtt{cmpCount}(L_1, L_2) = 0 \leq 0 = \#L_1 + \#L_2$.
\item[I.S.:] $\#L_1 + \#L_2 = n+1$.

             If either $L_1$ or $L_2$ is empty, then we do not need any comparisons in order to
             compute $\mathtt{merge}(L_1, L_2)$ and, therefore, we have
             \\[0.2cm]
             \hspace*{1.3cm}
             $\mathtt{cmpCount}(L_1,L_2) = 0 \leq \#L_1 + \#L_2$.
             \\[0.2cm]
             Next, let us assume that \\[0.2cm]
             \hspace*{1.3cm} $L_1 = [x] + R_1$ \quad and \quad $L_2 = [y] + R_2$.
             \\[0.2cm]
             We have to do a case distinction with respect to the relative size of $x$ and $y$.
             \begin{enumerate}
             \item $x \preceq y$.  Then we have \\[0.2cm]
                   \hspace*{1.3cm} 
                   $\mathtt{merge}\bigl([x] + R_1, [y] + R_2\bigr) = [x] + \mathtt{merge}\bigl(R_1, [y] + R_2\bigr)$. 
                   \\[0.2cm]
                   Therefore, we have \\
                   $\mathtt{cmpCount}(L_1, L_2) = 1 + \mathtt{cmpCount}(R_1, L_2) \stackrel{ih}{\leq} 1 + \#R_1 + \#L_2 = \#L_1 + \#L_2$.
             \item $\neg x \preceq y$.  This case is similar to the previous case. 
                   \qed
             \end{enumerate}
\end{enumerate}
\exercise
What is the form of the lists $L_1$ and $L_2$ that maximizes the value of 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{cmpCount}(L_1, L_2)$?
\\[0.2cm]
What is the value of $\mathtt{cmpCount}(L_1, L_2)$ in this case? \eox
\vspace*{0.3cm}

\noindent
Now we are ready to compute the complexity of \blue{merge sort} in the worst case.  Define
\\[0.2cm]
\hspace*{1.3cm}
$f(n)$  \texttt{:=} number of comparisons needed to sort a list $L$ of length $n$.
\\[0.2cm]
The algorithm \blue{merge sort} 
splits the list $L$ into two lists that have the length of $n\symbol{92}2$ or $n\symbol{92}2+1$, then sorts
these lists recursively, and finally merges the sorted lists.  Merging the two lists can be done with
at most $n$ comparisons.  Therefore, the function $f$ satisfies the recurrence relation
\\[0.2cm]
\hspace*{1.3cm}
$f(n) = 2 \cdot f(n \symbol{92} 2) + \Oh(n)$,
\\[0.2cm]
We can use the master theorem to get an upper bound for $f(n)$.  In the master theorem, we have
$\alpha = 2$, $\beta = 2$, and $\delta = 1$. Therefore,
\\[0.2cm]
\hspace*{1.3cm}
$\beta^\delta = 2^1 = 2 = \alpha$
\\[0.2cm]
and hence the master theorem shows that we have
\\[0.2cm]
\hspace*{1.3cm}
$f(n) \in \Oh\bigl(n \cdot \log_2(n) \bigr)$.
\\[0.2cm]
This result already shows that, for large inputs, \blue{merge sort} is considerably more efficient
than both \blue{insertion sort} and \blue{selection sort}.  However, if we want to compare
\blue{merge sort} with \blue{quick sort}, the result $f(n) \in \Oh\bigl(n \cdot \log_2(n) \bigr)$ is
not precise enough.  In order to arrive at a bound for the number of comparisons that is more precise,
we need to solve the recurrence equation given above.  To simplify things,  define
\\[0.2cm]
\hspace*{1.3cm}
$a_n := f(n)$ 
\\[0.2cm]
and assume that $n$ is a power of $2$, i.e.~we assume that
\\[0.2cm]
\hspace*{1.3cm}
$\ds n = 2^k$ \qquad for some $k \el \mathbb{N}$.
\\[0.2cm]
Let us define
\\[0.2cm]
\hspace*{1.3cm}
$b_k := a_n = a_{2^k}$.
\\[0.2cm]
First, we compute the initial value $b_0$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds b_0 = a_{2^0} = a_1 = 0$,
\\[0.2cm]
since we do not need any comparisons when sorting a list of length one.  Since merging two lists of
length $2^k$ needs at most  $2^k + 2^k = 2^{k+1}$ comparisons, $b_{k+1}$ can be upper bounded as follows:
\\[0.2cm]
\hspace*{1.3cm}
$b_{k+1} = 2 \cdot b_k + 2^{k+1}$. 
\\[0.2cm]
In order to solve this recurrence equation, we divide the equation by $2^{k+1}$.
This yields
\\[0.2cm]
\hspace*{1.3cm}
$\bruch{b_{k+1}}{2^{k+1}} = \bruch{b_k}{2^k} + 1$.
\\[0.2cm]
Next, we define
\\[0.2cm]
\hspace*{1.3cm}
$c_k := \bruch{b_k}{2^k}$.
\\[0.2cm]
Then, we get the following equation for $c_k$:
\\[0.2cm]
\hspace*{1.3cm}
$c_{k+1} = c_k + 1$.
\\[0.2cm]
Since $b_0 = 0$, we also have $c_0 = 0$.  Hence, the solution of the recurrence equation for $c_k$
is given as
\\[0.2cm]
\hspace*{1.3cm}
$c_k := k$.
\\[0.2cm]
Substituting this value into the defining equation for $c_k$ we conclude that
\\[0.2cm]
\hspace*{1.3cm}
$b_k = 2^k \cdot k$.
\\[0.2cm]
Since $n = 2^k$ implies $k = \log_2(n)$ and $a_n = b_k$, we have found that
\\[0.2cm]
\hspace*{1.3cm}
$a_n = n \cdot \log_2(n)$. 


\subsection{Implementing Merge Sort for Arrays}
All the implementations of the \textsc{SetlX} programs presented up to now are quite inefficient.  The
reason is that, in \textsc{SetlX}, lists are internally represented as arrays.  Therefore, when
we evaluate an expression of the form 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{[x] + R}
\\[0.2cm]
the following happens:
\begin{enumerate}
\item A new array is allocated.  This array will later hold the resulting list.
\item The element \texttt{x} is copied to the beginning of this array.
\item The elements of the list \texttt{R} are copied to the positions following \texttt{x}.
\end{enumerate}
Therefore, evaluating \texttt{[x] + R} for a list \texttt{R} of length $n$ requires $\Oh(n)$ data
movements.  This is very wasteful.  In order to arrive at an
implementation that is more efficient we need to make use of the fact that lists are represented as arrays.
Figure \ref{fig:merge-sort-array.stlx} on page \pageref{fig:merge-sort-array.stlx} presents
an implementation of \blue{merge sort} that treats the list $L$ that is to be sorted as an array.


\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.4cm,
                  xleftmargin   = 1.3cm,
                  xrightmargin  = 1.3cm
                ]
    sort := procedure(rw L) {
        A := L;
        mergeSort(L, 1, #L + 1, A); 
    };
    mergeSort := procedure(rw L, start, end, rw A) {
        if (end - start < 2) { return; }
        middle := (start + end) \ 2;
        mergeSort(L, start,  middle, A);  
        mergeSort(L, middle, end   , A);   
        merge(L, start, middle, end, A); 
    };
    merge := procedure(rw L, start, middle, end, rw A) {    
        for (i in [start .. end-1]) { A[i] := L[i]; }
        idx1 := start;
        idx2 := middle;
        i    := start;
        while (idx1 < middle && idx2 < end) {
            if (A[idx1] <= A[idx2]) {
                L[i] := A[idx1]; idx1 += 1;
            } else {
                L[i] := A[idx2]; idx2 += 1;
            }
            i += 1; 
        }
        while (idx1 < middle) { 
            L[i] := A[idx1]; idx1 += 1; i += 1; 
        }
        while (idx2 < end   ) { 
            L[i] := A[idx2]; idx2 += 1; i += 1; 
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{An array based implementation of \blue{merge sort}.}
  \label{fig:merge-sort-array.stlx}
\end{figure}
We discuss the implementation shown in Figure \ref{fig:merge-sort-array.stlx} line by line.
\begin{enumerate}
\item In line 1 the keyword ``\texttt{rw}'' specifies that the parameter $\mathtt{L}$ is a
      \blue{\underline{r}ead-\underline{w}rite} parameter.  Therefore, changes to $\mathtt{L}$ remain
      visible after $\texttt{sort}(\mathtt{L})$ has returned.  This is also the reason that the procedure 
      \texttt{sort} does not return a result.  Instead, the evaluation of the expression
      $\mathtt{sort}(\mathtt{L})$ has the side effect of sorting the list $\mathtt{L}$.
\item The purpose of the assignment ``\texttt{A := L;}'' in line 2 is to create an auxiliary array
      $\texttt{A}$.  This auxiliary array is needed in the procedure
      \texttt{mergeSort} called in line 3.
\item The procedure \texttt{mergeSort} defined in line 5 is called with 4 arguments.
      \begin{enumerate}
      \item The first parameter $\texttt{L}$ is the list that is to be sorted.
      \item However, the task of \texttt{mergeSort} is not to sort the entire list $\texttt{L}$ but only
            the part of $\texttt{L}$ that is given as
            \\[0.2cm]
            \hspace*{1.3cm} 
            \texttt{L[start..end-1]}. 
            \\[0.2cm]
            Hence, the parameters \texttt{start} and \texttt{end} are indices specifying the 
            subarray that needs to be sorted.
      \item The final parameter $\texttt{A}$ is used as an auxiliary array.  This array is needed
            as temporary storage and it needs to have the same size as the list $\texttt{L}$.
      \end{enumerate} 
\item Line 6 deals with the case that the sublist of $\texttt{L}$ that needs to be sorted has at most one element.  
      In this case, there is nothing to do as any such list is already sorted.
\item In line 7 we compute the index pointing to the middle element of the list $\texttt{L}$ using the
      formula \\[0.2cm]
      \hspace*{1.3cm} 
      \texttt{middle = (start + end) \symbol{92} 2;} 
      \\[0.2cm]
      This way, the list $\texttt{L}$ is split into the lists 
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{L[start..middle-1]} \quad and \quad \texttt{L[middle..end-1]}.
      \\[0.2cm]
      These two lists have approximately the same size which is about half the size of the list $\texttt{L}$.
\item Next, the lists \texttt{L[start..middle-1]} and \texttt{L[middle..end-1]} are sorted
      recursively in line 8 and 9, respectively.
\item The call to \texttt{merge} in line 10 merges these lists.
\item The procedure \texttt{merge} defined in line 12 has 5 parameters: 
      \begin{enumerate}
      \item The first parameter $\texttt{L}$ is the list that contains the two sublists that have to be merged.
      \item The parameters \texttt{start}, \texttt{middle}, and \texttt{end} specify the sublists
            that have to be merged.  The first sublist is 
            \\[0.2cm]
            \hspace*{1.3cm} 
            \texttt{L[start..middle-1]}, 
            \\[0.2cm]
            while the second sublist is \\[0.2cm]
            \hspace*{1.3cm} 
            \texttt{L[middle..end-1]}. 
      \item The final parameter $\texttt{A}$ is used as an auxiliary array.  It needs to be a list of the
            same size as the list $\texttt{L}$.
      \end{enumerate}
\item The function \texttt{merge} assumes that the sublists 
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{L[start..middle-1]} \quad and \quad \texttt{L[middle..end-1]} 
      \\[0.2cm]
      are already sorted.  The merging of these sublists works as follows:
      \begin{enumerate}
      \item First, line 13 copies the sublists into the auxiliary array $\texttt{A}$.
      \item In order to merge the two sublists stored in $\texttt{A}$ into the list $\texttt{L}$ we define
            three indices: 
            \begin{itemize}
            \item \texttt{idx1} points to the next element of the first sublist stored in $\texttt{A}$.
            \item \texttt{idx2} points to the next element of the second sublist stored in $\texttt{A}$.
            \item \texttt{i} points to the position in the list $\texttt{L}$ where we have to put the next
                       element.
            \end{itemize}
      \item As long as neither the first nor the second sublist stored in $\texttt{A}$ have been exhausted
            we compare in line 17 the elements from these sublists and then copy the smaller of these
            two elements into the list $\texttt{L}$ at position \texttt{i}.
            In order to remove this element from the corresponding sublist in $\texttt{A}$ we just need to
            increment the corresponding index pointing to the beginning of this sublist.
      \item If one of the two sublists gets empty while the other sublist still has elements, then we have
            to copy the remaining elements of the non-empty sublist into the list $\texttt{L}$.
            The \texttt{while}-loop in line 24 covers the case that the second sublist is exhausted before 
            the first sublist, while the \texttt{while}-loop in line 25 covers the case that the first
            sublist is exhausted before the second sublist.
      \end{enumerate}
\end{enumerate}

\subsection{An Iterative Implementation of Merge Sort}

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    sort := procedure(rw L) {
        A := L;
        mergeSort(L, A);
    };
    mergeSort := procedure(rw L, rw A) {
        n := 1;
        while (n < #L) {
            k := 0;
            while (n * (k + 1) + 1 <= #L) {
                merge(L, n*k+1, n*(k+1)+1, min([n*(k+2), #L])+1, A);
                k += 2;    
            }
            n *= 2;
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{A non-recursive implementation of \blue{merge sort}.}
  \label{fig:merge-sort-nr.stlx}
\end{figure}

\noindent
The implementation of \blue{merge sort} shown in Figure \ref{fig:merge-sort-array.stlx} on page
\pageref{fig:merge-sort-array.stlx} is recursive.  Unfortunately, the efficiency of a recursive
implementation of \blue{merge sort} is suboptimal.  The reason is that function calls are quite
costly since the arguments of the function have to be placed on a stack.  As a recursive
implementation has lots of function calls, it is considerably less efficient than an iterative
implementation.  Therefore, we present an iterative implementation of \blue{merge sort} in Figure
\ref{fig:merge-sort-nr.stlx} on page \pageref{fig:merge-sort-nr.stlx}.

Instead of recursive calls of the function \texttt{mergeSort}, this implementation has two nested 
\texttt{while}-loops.  The idea is to first split the list $\texttt{L}$ into sublists of length 1.
Obviously, these sublists are already sorted.  Next, we merge pairs of these lists into lists of
length 2.  After that, we take pairs of lists of length 2 and merge them into sorted lists of length
4. Proceeding in this way we generate sorted lists of length
$8$, $16$, $\cdots$.  This algorithm only stops when the list $\texttt{L}$ itself is sorted.

The precise working of this implementation gets obvious if we formulate the invariants of the
\texttt{while}-loops.  The invariant of the outer loop states that all sublists of $\texttt{L}$ 
that have the form
\\[0.2cm]
\hspace*{1.3cm}
\texttt{L[n*k+1..n*(k+1)]}
\\[0.2cm]
are already sorted.  These sublists have a length of $n$.  It is the task of the outer while loop to build
pairs of sublists of this kind and to merge them into a sublist of length $2 \cdot n$.

In the expression $\texttt{L[n*k+1 .. n*(k+1)]}$ the variable $k$ denotes a natural
number that is used to numerate the sublists.  The index $k$ of the first sublists is $0$ and
therefore this sublists has the form
\\[0.2cm]
\hspace*{1.3cm}
\texttt{L[1..n]},
\\[0.2cm]
while the second sublist is given as
\\[0.2cm]
\hspace*{1.3cm}
\texttt{L[n+1..2*n]}.
\\[0.2cm]
It is possible that the last sublist has a length that is less than $n$.  This happens if the length
of $\texttt{L}$ is not a multiple of $n$.  Therefore, the third argument of the call to \texttt{merge}
in line 10 is the minimum of $n\cdot(k+2)$ and $\texttt{\#L}$.

\subsection{Further Improvements of Merge Sort}
The implementation given above can still be improved in a number of ways.  
\href{http://c2.com/cgi/wiki?TimPeters}{Tim Peters} has used a number of tricks to improve the
practical performance of \blue{merge sort}.  The resulting algorithm is known as
\href{http://en.wikipedia.org/wiki/Timsort}{Timsort}.

The starting point of the development of \blue{Timsort} was the observation that the input arrays 
given to a sorting procedure often contain subarrays that are already sorted, either ascendingly or
descendingly.   For this reason, \blue{Timsort} uses the following tricks:
\begin{enumerate}
\item First, Timsort looks for subarrays that are already sorted.
      If a subarray is sorted descendingly, this subarray is reversed.
\item Sorted subarrays that are too small (i.~e.~have less than 32 elements) are extended
      to sorted subarrays to have a length that is at least 32.  In order to sort these subarrays,
      \blue{insertion sort} is used.  The reason is that \blue{insertion sort} is very fast for
      arrays that are already partially sorted.  The version of \blue{insertion sort} that is used is called
      \blue{binary insertion sort} since it uses 
      \href{http://en.wikipedia.org/wiki/Binary_search}{\blue{binary search}} to insert the elements
      into the array.
\item The algorithm to merge two sorted lists can be improved by the following observation: If we
      want to merge the arrays
      \\[0.2cm]
      \hspace*{1.3cm}
      $[x] + \mathtt{R}$ \quad and \quad $\mathtt{L}_1 + [y] + \mathtt{L}_2$
      \\[0.2cm]
      and if $y$ is less than $x$, then all elements of the list $\mathtt{L}_1$ are also less than $x$.
      Therefore, there is no need to compare these elements with $x$ one by one.  
\end{enumerate}
Timsort uses some more tricks, but unfortunately we don't have the time to discuss all of them.
Originally, Tim Peters developed Timsort for the programming language \textsl{Python}.  Today,
\blue{Timsort} is also part of the \textsl{Java} library, the source code is available online at
\\[0.2cm]
\hspace*{0.3cm}
\href{http://hg.openjdk.java.net/jdk10/jdk10/jdk/file/ffa11326afd5/src/java.base/share/classes/java/util/ComparableTimSort.java}{\texttt{http://hg.openjdk.java.net/jdk10/jdk10/jdk/file/ffa11326afd5/\\
\hspace*{1.5cm}
    src/java.base/share/classes/java/util/ComparableTimSort.java}}
\\[0.2cm]
Timsort is also used on the Android platform.

\section{\href{http://en.wikipedia.org/wiki/Quicksort}{Quicksort}}
In 1961, \href{http://en.wikipedia.org/wiki/Tony_Hoare}{C.A.R.~Hoare} published the
\blue{quicksort}  
algorithm \cite{hoare:61}.  The basic idea is as follows:
\begin{enumerate}
\item If the list $\mathtt{L}$ that is to be sorted is empty, we return $\mathtt{L}$: 
      \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{sort}([]) = []$.
\item Otherwise, we have $\mathtt{L} = [\mathtt{x}|\mathtt{R}]$.  In this case, we split $\mathtt{R}$ into two lists $\mathtt{S}$ and $\mathtt{B}$.
      The list $\mathtt{S}$ (the letter $\mathtt{S}$ stands for \underline{s}mall) contains all those elements of $\mathtt{R}$ that are less
      or equal than $\mathtt{x}$,     while $\mathtt{B}$ (the letter $\mathtt{B}$ stands for \underline{b}ig) contains
      those elements of $\mathtt{R}$ that are bigger than $\mathtt{x}$.  These lists are computed as follows:
      \begin{enumerate}
      \item $\mathtt{S} := [\mathtt{y} \in \mathtt{R} \mid \mathtt{y} \leq \mathtt{x}]$,
      \item $\mathtt{B} := [\mathtt{y} \in \mathtt{R} \mid \mathtt{y} > \mathtt{x}]$.
      \end{enumerate}
      The process of splitting the list $\mathtt{R}$ into the lists $\mathtt{S}$ and $\mathtt{B}$
      is called \blue{partitioning}.  After partitioning the list $\mathtt{R}$ into the lists $\mathtt{S}$ and $\mathtt{B}$, these lists are sorted
      recursively.  Then, the result is computed by putting $\mathtt{x}$ between the lists $\mathtt{sort}(\mathtt{S})$ and $\mathtt{sort}(\mathtt{B})$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{sort}([\mathtt{x}|\mathtt{R}]) = \mathtt{sort}(\mathtt{S}) + [\mathtt{x}] + \mathtt{sort}(\mathtt{B})$.
\end{enumerate}
Figure \ref{fig:quick-sort.stlx} on page \pageref{fig:quick-sort.stlx} shows how these equations can
be implemented in \textsc{SetlX}.

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm
                ]
    sort := procedure(L) {
        match (L) {
            case []   : return [];
            case [x|R]: S := [y: y in R | y <= x];
                        B := [y: y in R | y >  x];
                        return sort(S) + [x] + sort(B);
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{The \blue{quicksort} algorithm.}
  \label{fig:quick-sort.stlx}
\end{figure}

\subsection{Complexity}
Next, we investigate the computational complexity of \blue{quicksort}.
Our goal is to compute the number of comparisons that are needed when
$\mathtt{sort}(\mathtt{L})$ is computed for a list $\mathtt{L}$ of length $n$.  In order to compute this number we
first investigate how many comparisons are needed in order to partition the list $\mathtt{R}$
into the lists $\mathtt{S}$ and $\mathtt{B}$.  As these lists are defined as 
\\[0.2cm]
\hspace*{1.3cm}
 $\mathtt{S} := [\mathtt{y} \in \mathtt{R} \mid \mathtt{y} \leq \mathtt{x}]$ \quad and \quad
 $\mathtt{B} := [\mathtt{y} \in \mathtt{R} \mid \mathtt{y} > \mathtt{x}]$,
\\[0.2cm]
it is obvious that each element of $\mathtt{R}$ has to be compared with $\mathtt{x}$.
Therefore, we need $n$ comparisons to compute $\mathtt{S}$ and $\mathtt{B}$.  Since $\mathtt{S}$ and
$\mathtt{B}$ are computed independently, the implementation given above 
would really need $2 \cdot n$ comparisons.  However, a moments thought reveals that we can compute $\mathtt{S}$ and $\mathtt{B}$
with just $n$ comparisons if the two lists are computed simultaneously.
Next, we investigate the worst case complexity of quicksort.

\subsubsection{Worst Case Complexity}
Let us denote the number of comparisons needed to evaluate $\mathtt{sort}(\mathtt{L})$ for a list $\mathtt{L}$ of
length $n$ in the worst case as $a_n$.  The worst case occurs if the partitioning
returns a pair of lists $\mathtt{S}$ and $\mathtt{B}$ such that
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{S} = []$,
\\[0.2cm]
i.e.~all elements of $\mathtt{R}$ are bigger than $x$.  Then, we have
\\[0.2cm]
\hspace*{1.3cm}
$a_n = a_{n-1} + n - 1$. 
\\[0.2cm]
The term $n-1$ is due to the $n-1$ comparisons needed for the partitioning of $\mathtt{R}$
and the term $a_{n-1}$ is the number of comparisons needed for the recursive evaluation of $\mathtt{sort}(\mathtt{B})$.

The initial condition is $a_1 = 0$, since we do not need any comparisons to sort a list
containing only one element.
Hence the recurrence relation can be solved as follows:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcl}
  a_n & = & a_{n-1} + (n-1) \\[0.2cm]
      & = & a_{n-2} + (n-2) + (n-1) \\[0.2cm]
      & = & a_{n-3} + (n-3) + (n-2) + (n-1) \\
      & = & \vdots \\
      & = & a_{1} + 1 + 2 + \cdots  + (n-2) + (n-1) \\[0.2cm]
      & = & 0 + 1 + 2 + \cdots  + (n-2) + (n-1) \\[0.2cm]
      & = & \ds \sum\limits_{i=0}^{n-1} i  =  \frac{1}{2} \cdot n \cdot(n - 1) =
            \frac{1}{2} \cdot n^2 - \frac{1}{2} \cdot n \\[0.5cm]
      & \in & \Oh(n^2)
\end{array}
$
\\[0.2cm]
This shows that in the worst case, the number of comparisons is as big as it is in the worst case of 
\blue{insertion sort}.  However, with quicksort the worst case occurs if we try to sort a list $\mathtt{L}$ that is
already sorted. 


\subsubsection{Average Complexity}
By this time you probably wonder why the algorithm has been called \blue{quicksort} since, in the worst case,
it is much slower as \blue{merge sort}.  To understand what is really going on, we define
\\[0.2cm]
\hspace*{1.3cm}
$d_n$ \texttt{:=} \underline{avera}g\underline{e} number of comparisons to sort a list $L$ of $n$ elements via quicksort.
\\[0.2cm]
We will show that $d_n \in \Oh\bigl(n \cdot \log_2(n)\bigr)$.  Let us first note the following: If $\mathtt{L}$
is a list of $n+1$ elements, then the number of elements of the  
list $\mathtt{S}$ that are smaller than or equal to the pivot element $\mathtt{x}$ is a member of the set 
$\{0,1,2,\cdots,n\}$.  If the length of $\mathtt{S}$ is $i$ and the length of $\mathtt{L}$ is $n+1$, then the length
of the list $\mathtt{B}$ of those elements, that are bigger than $\mathtt{x}$ is $n-i$.  Therefore, if $\#\mathtt{S} = i$, then on average we need
\\[0.2cm]
\hspace*{1.3cm} $d_i + d_{n-i}$ \\[0.2cm]
comparisons to sort the lists $\mathtt{S}$ and $\mathtt{B}$ recursively.  If we take the average over all possible values of
 $i = \#S$ then, since $i \el\{0,1,\cdots,n\}$ and this set has $n+1$ elements, we get the following
 recurrence relation for $d_{n+1}$: 
\\[0.2cm]
\hspace*{1.3cm}
$\ds d_{n+1} = n + \frac{1}{n+1} \cdot \sum\limits_{i=0}^n (d_i + d_{n-i}) $ \hspace*{\fill} (1) 
\\[0.2cm] 
Here, the term $n$ accounts for the number of comparisons needed to partition the list $\mathtt{R}$.
In order to simplify the recurrence relation (1) we note that
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
 \ds
 \sum\limits_{i=0}^n a_{n-i} & = & a_n + a_{n-1} + \cdots + a_1 + a_0 \\[0.2cm]
                    & = & a_0 + a_1 + \cdots + a_{n-1} + a_n \\[0.2cm]
                    & = & \ds \sum\limits_{i=0}^n a_i 
\end{array}
$
\\[0.2cm]
holds for any sequence $(a_n)_{n\in\mathbb{N}}$.
This observation can be used to simplify the recurrence relation (1) as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds d_{n+1} = n + \frac{2}{n+1} \cdot \sum\limits_{i=0}^n d_i$. \hspace*{\fill} (2)
\\[0.2cm] 
In order to solve this recurrence relation we substitute $n \mapsto n-1$ and arrive at
\\[0.2cm]
\hspace*{1.3cm}
$\ds d_{n} = n-1 + \frac{2}{n} \cdot \sum\limits_{i=0}^{n-1} d_i$.  \hspace*{\fill} (3)
\\[0.2cm]
Next, we multiply equation (3) with $n$ and equation (2) with $n+1$.  This yields the equations
\\[0.2cm]
\hspace*{1.3cm}
$\ds n \cdot d_{n} = n\cdot(n-1) + 2 \cdot \sum\limits_{i=0}^{n-1} d_i$,            \hspace*{\fill}
(4) 
\\[0.2cm]
\hspace*{1.3cm}
$\ds (n+1)\cdot d_{n+1}  =  (n+1)\cdot n + 2 \cdot \sum\limits_{i=0}^n d_i$.   \hspace*{\fill} (5) 
\\[0.2cm]
We take the difference of  equation (5) and (4) and note that the summations cancel except for
the term $2\cdot d_{n}$.  This leads to
\\[0.2cm]
\hspace*{1.3cm}
$(n+1)\cdot d_{n+1} - n \cdot \ds d_{n} = (n+1)\cdot n - n \cdot (n-1) + 2 \cdot d_{n}$.
\\[0.2cm] 
This equation can be simplified as
\\[0.2cm]
\hspace*{1.3cm}
$(n+1)\cdot d_{n+1} = (n+2) \cdot \ds d_{n} + 2 \cdot n$.
\\[0.2cm] 
In order to exhibit the true structure of this equation we divide by $(n+1) \cdot(n+2)$ and
get 
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{1}{n+2} \cdot d_{n+1} = \frac{1}{n+1}\cdot d_{n} + \frac{2\cdot n}{(n+1)\cdot(n+2)}$. \hspace*{\fill} (6) 
\\[0.2cm]
In order to simplify this equation, let us define
\\[0.2cm]
\hspace*{1.3cm}
$\ds a_n = \frac{d_n}{n+1}$. 
\\[0.2cm] 
Substituting this into equation $(6)$ yields
\\[0.2cm]
\hspace*{1.3cm}
$\ds a_{n+1} = a_{n} + \frac{2\cdot n}{(n+1)\cdot(n+2)}$.  \hspace*{\fill} (7)
\\[0.2cm]
We proceed by computing the 
\href{http://en.wikipedia.org/wiki/Partial_fraction}{partial fraction decomposition}
of the fraction
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{2\cdot n}{(n+1)\cdot(n+2)}$.
\\[0.2cm] 
In order to so, we use the \href{http://en.wikipedia.org/wiki/Ansatz}{ansatz}
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{2\cdot n}{(n+1)\cdot(n+2)} = \frac{\alpha}{n+1} + \frac{\beta}{n+2}$.
\\[0.2cm] 
Multiplying this equation with $(n+1) \cdot (n+2)$ yields
\\[0.2cm]
\hspace*{1.3cm}
$ 2\cdot n = \alpha \cdot (n+2) + \beta \cdot (n+1)$.
\\[0.2cm]
Grouping similar terms, this can be simplified as follows:
\\[0.2cm]
\hspace*{1.3cm}
$2\cdot n = (\alpha + \beta) \cdot n + 2 \cdot \alpha  + \beta$.
\\[0.2cm]
Since this has to hold for every $n \in \mathbb{N}$ we must have:
\begin{eqnarray*}
  2 & = & \alpha + \beta, \\
  0 & = & 2 \cdot \alpha + \beta.
\end{eqnarray*}
If we subtract the first equation from the second equation we arrive at
 $\alpha = -2$.  Substituting this into the first equation gives $\beta = 4$.
Hence, the equation (6) can be written as
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{1}{n+2} \cdot d_{n+1} = \frac{1}{n+1}\cdot d_{n} - \frac{2}{n+1} + \frac{4}{n+2}$.  
\\[0.2cm]  
Then equation (7) is simplified to
\\[0.2cm]
\hspace*{1.3cm}
$\ds a_{n+1} = a_{n} - \frac{2}{n+1} + \frac{4}{n+2}$.
\\[0.2cm] 
Substituting $n \mapsto n-1$ simplifies this equation: 
\\[0.2cm]
\hspace*{1.3cm}
$\ds a_{n} = a_{n-1} - \frac{2}{n} + \frac{4}{n+1}$,
\\[0.2cm] 
This equation can be rewritten as a sum.  Since $a_0 = \ds\frac{d_0}{1} = 0$
we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds a_{n} = 4 \cdot \sum\limits_{i=1}^n \frac{1}{i+1} - 2 \cdot \sum\limits_{i=1}^n \frac{1}{\,i\,}$.  
\\[0.2cm]
Let us simplify this sum:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcl}
 a_{n} & = & \ds 4 \cdot \sum_{i=1}^n \frac{1}{i+1} - 2 \cdot \sum_{i=1}^n \frac{1}{\,i\,} \\[0.5cm]
       & = & \ds 4 \cdot \sum_{i=2}^{n+1} \frac{1}{\,i\,} - 2 \cdot \sum_{i=1}^n \frac{1}{\,i\,} \\[0.5cm]
       & = & \ds 4 \cdot \frac{1}{n+1} - 4 \cdot \frac{1}{1} + 4 \cdot \sum_{i=1}^{n} \frac{1}{\,i\,} - 2 \cdot \sum_{i=1}^n \frac{1}{\,i\,} \\[0.5cm]
       & = & \ds 4 \cdot \frac{1}{n+1} - 4 \cdot \frac{1}{1} + 2 \cdot \sum_{i=1}^{n} \frac{1}{\,i\,}  \\[0.5cm]
       & = & \ds - \frac{4 \cdot n}{n+1}  + 2 \cdot \sum_{i=1}^{n} \frac{1}{\,i\,}  
\end{array}
$
\\[0.2cm]
In order to finalize our computation we have to compute an approximation for the sum
\\[0.2cm]
\hspace*{1.3cm}
$H_n = \ds\sum\limits_{i=1}^{n}\frac{1}{\,i\,}$.
\\[0.2cm] 
The number $H_n$ is known in mathematics as the $n$-th 
\href{http://en.wikipedia.org/wiki/Harmonic_number}{harmonic number}.
\href{http://en.wikipedia.org/wiki/Leonhard_Euler}{Leonhard Euler} (1707 -- 1783) was able to prove
that the harmonic numbers can be approximated as
\\[0.2cm]
\hspace*{1.3cm}
$\ds H_n = \ln(n) + \gamma + \Oh\Bigl(\frac{1}{\,n\,}\Bigr)$. 
\\[0.2cm] 
In the formula approximating the harmonic number $H_n$, $\gamma$ is the
\href{http://en.wikipedia.org/wiki/Euler-Mascheroni_constant}{Euler-Mascheroni} constant and has the 
value
\\[0.2cm]
\hspace*{1.3cm}
$\gamma = 0.5772156649 \cdots$.
\\[0.2cm]
Therefore, we have found the following approximation for $a_n$:
\\[0.2cm]
\hspace*{1.3cm}
$\ds a_n = - \frac{4 \cdot n}{n+1}  + 2 \cdot \ln(n) + \Oh(1) =  2 \cdot \ln(n) + \Oh(1)$,
\quad as \quad $\ds \frac{4 \cdot n}{n+1} \in \Oh(1)$.
\\[0.2cm]
Since we have $d_n = (n+1) \cdot a_{n}$ we can conclude that
\begin{eqnarray*}  
 d_n & = &  2 \cdot(n+1) \cdot H_n + \Oh(n) \\
     & = & 2 \cdot n \cdot \ln(n) + \Oh(n)
\end{eqnarray*}
holds.  Let us compare this result with the number of comparisons needed for \blue{merge sort}.
We have seen previously that \blue{merge sort} needs
\\[0.2cm]
\hspace*{1.3cm} $n \cdot \log_2(n) + \Oh(n)$ \\[0.2cm]
comparisons in order to sort a list of $n$ elements.  Since we have $\ln(n) = \ln(2) \cdot \log_2(n)$
we conclude that the average case of \blue{quicksort} needs
 \\[0.2cm]
\hspace*{1.3cm} $2 \cdot \ln(2) \cdot n \cdot \log_2(n) + \Oh(n)$ \\[0.2cm]
comparisons and hence on average \blue{quicksort} needs  $2 \cdot \ln(2) \approx 1.39$ times as many comparisons as
\blue{merge sort}.  


\subsection{Implementing Quicksort for Arrays}
Next, we show how \blue{quicksort}  is implemented using arrays instead of lists.  We are following the scheme
of Nico Lomuto \cite{cormen:09}.
Figure \ref{fig:quick-sort-array.stlx} on page \pageref{fig:quick-sort-array.stlx} shows this implementation. 

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    sort := procedure(rw L) {
        quickSort(1, #L, L);
    };
    quickSort := procedure(a, b, rw L) {
        if (b <= a) {
            return; // at most one element, nothing to do
        }
        m := partition(a, b, L);  // m is the split index
        quickSort(a, m - 1, L);
        quickSort(m + 1, b, L);
    };
    partition := procedure(start, end, rw L) {
        pivot := L[end];
        left  := start - 1;
        for (idx in [start .. end-1]) {
            if (L[idx] <= pivot) {
                left += 1;
                swap(left, idx, L);
            }
        }
        swap(left + 1, end, L);
        return left + 1;
    };
    swap := procedure(x, y, rw L) {
        [ L[x], L[y] ] := [ L[y], L[x] ];
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{An implementation of \blue{quicksort} based on arrays.}
  \label{fig:quick-sort-array.stlx}
\end{figure}

\begin{enumerate}
\item Contrary to the array based implementation of \blue{merge sort}, we do not need an auxiliary
      array.  This is one of the main advantages of \blue{quicksort} over \blue{merge sort}.
\item The function $\mathtt{sort}$ is reduced to a call of $\mathtt{quickSort}$.  This function
      takes the parameters $\mathtt{a}$, $\mathtt{b}$, and $\mathtt{L}$.  
      \begin{enumerate}
      \item $\mathtt{a}$ specifies the index of the first element of the subarray that needs to be
            sorted.
      \item $\mathtt{b}$ specifies the index of the last element of the subarray that needs to be
            sorted. 
      \item $\mathtt{L}$ is the array that needs to be sorted.
      \end{enumerate}
      Calling $\mathtt{quickSort}(\mathtt{a}, \mathtt{b}, \mathtt{L})$ sorts the subarray \\[0.2cm]
      \hspace*{1.3cm} 
      $\bigl[\mathtt{L}[\mathtt{a}], L[\mathtt{a}+1], \cdots, L[\mathtt{b}]\bigr]$
      \\[0.2cm]
      of the array $\mathtt{L}$, i.~e.~after that call we expect to have\\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{L}[\mathtt{a}] \preceq \mathtt{L}[\mathtt{a}+1] \preceq \cdots \preceq \mathtt{L}[\mathtt{b}]$.
      \\[0.2cm]
      The implementation of the function $\mathtt{quickSort}$
      is quite similar to the list implementation.  The main difference is that the function
      $\mathtt{partition}$, that is called in line 8, redistributes the elements of $\mathtt{L}$:
      All elements that are less or equal than the \blue{pivot element} $\mathtt{L[m]}$
      are stored at indexes that are smaller than the index $\mathtt{m}$, while the remaining elements will 
      be stored at indexes that are bigger than $\mathtt{m}$.  The pivot element itself will be stored at the
      index $\mathtt{m}$. 
\item The difficult part of the implementation of \blue{quicksort} is the implementation of the
      function $\mathtt{partition}$ that is shown beginning in line 12.
      The \texttt{for} loop in line 15 satisfies the following invariants.
      \begin{enumerate}
      \item $\forall i \in \{ \mathtt{start}, \cdots, \mathtt{left} \} : \mathtt{L}[i] \leq \mathtt{pivot}$.

            All elements in the subarray $\mathtt{L[start..left]}$ are less or equal than the pivot element.
      \item $\forall i \in \{ \mathtt{left}+1,\cdots,\mathtt{idx}-1\}:\mathtt{pivot} < \mathtt{L}[i]$.

            All elements in the subarray $\mathtt{L}[\mathtt{left}+1\;..\;\mathtt{idx}-1]$ are greater than the pivot
            element.
      \item $\mathtt{pivot} = \mathtt{L[end]}$

            The pivot element itself is at the end of the array.
      \end{enumerate}
      Observe how the invariants (a) and (b) are maintained:
      \begin{enumerate}
      \item Initially, the invariants are true because the corresponding sets are empty.
            At the start of the \texttt{for}-loop we have
            \\[0.2cm]
            \hspace*{1.3cm}
            $\{ \mathtt{start}, \cdots, \mathtt{left} \} = \{ \mathtt{start}, \cdots, \mathtt{start} - 1\} = \{\}$
            \\
            and
            \\
            \hspace*{1.3cm}
            $\{ \mathtt{left}+1,\cdots,\mathtt{idx}-1\} =  \{ \mathtt{start},\cdots,\mathtt{start}-1\}=\{\}$.
      \item If the element $\mathtt{L[idx]}$ is less than the
            pivot element, it need to become part of the subarray $\mathtt{L}[\mathtt{start}\;..\;\mathtt{left}]$.  In order to
            achieve this, it is placed at the position $\mathtt{L}[\mathtt{left}+1]$.  The element that has been at
            that position is part of the subarray $\mathtt{L}[\mathtt{left}+1\; ..\; \mathtt{idx}-1]$ and therefore, most of the times,\footnote{It is not always greater than the pivot element
      because the subarray $\mathtt{L}[\mathtt{left}+1\;..\;\mathtt{idx}-1]$ might well be empty.}
            it is greater than the pivot element.  
            Hence we append this element to the end of the subarray
            $\mathtt{L}[\mathtt{left}+1\;..\;\mathtt{idx}-1]$.  After incrementing the index $\mathtt{left}$,
            both the placing of the element $\mathtt{L[idx]}$ at position $\mathtt{left}+1$ and the appending
            of the element $\mathtt{L}[\mathtt{left}+1]$ to the end of the subarray
            $\mathtt{L}[\mathtt{left}+1\;..\;\mathtt{idx}-1]$ is achieved by the statement
            \\[0.2cm]
            \hspace*{1.3cm}
            \texttt{swap(left, idx, L}.            
      \end{enumerate}
      Once the \texttt{for} loop in line 15 terminates, the call to $\mathtt{swap}$ in line 21 moves
      the pivot element into its correct position and returns the index where the pivot element has been
      placed. 
\end{enumerate}


\subsection{Improvements for Quicksort}
There are a number of tricks that can be used to increase the efficiency of \blue{quicksort}.
\begin{enumerate}
\item Instead of taking the first element as the pivot element, use three elements from the list
      $\mathtt{L}$ that is to be sorted.  For example, take the first element, the last element, and an
      element from the middle of the list.  Now compare these three elements and take that element as
      a pivot that is between the other two elements.

      The advantage of this strategy is that the worst case performance is much less likely to occur.  In
      particular,  using this strategy the worst case won't occur for a list that is already
      sorted.
\item If a sublist contains fewer than 10 elements, use \blue{insertion sort} to sort this sublist.

      The paper ``\blue{Engineering a Sort Function}'' by Jon L.~Bentley and M.~Douglas McIlroy
      \cite{bentley:93} describes the previous two improvements.
\item In order to be sure that the average case analysis of \blue{quicksort} holds we can randomly
      \blue{shuffle} the list $L$ that is to be sorted.  This approach is advocated by Sedgewick
      \cite{sedgewick:2011}.  In \textsc{SetlX} this is quite easy as
      there is a predefined function \texttt{shuffle} that takes a list and shuffles it randomly.
      For example, the expression
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{shuffle([1..10]);}
      \\[0.2cm]
      might return the result
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{[1, 9, 8, 5, 2, 10, 6, 3, 4, 7]}.
\item In 2009, Vladimir Yaroslavskiy introduced \blue{dual pivot quicksort} \cite{yaroslavskiy:2009}.  His paper can be
      downloaded at the following address:
      \\[0.2cm]
      \hspace*{-0.3cm}
      \href{http://codeblab.com/wp-content/uploads/2009/09/DualPivotQuicksort.pdf}{\texttt{http://codeblab.com/wp-content/uploads/2009/09/DualPivotQuicksort.pdf}}
      \\[0.2cm]
      The main idea of Yaroslavskiy is to use two pivot elements $x$ and $y$.  For example, we can
      define
      \\[0.2cm]
      \hspace*{1.3cm}
      $x := \mathtt{L}[1]$ \quad and \quad $y := \mathtt{L}[\#\mathtt{L}]$,
      \\[0.2cm]
      i.e.~we take $x$ as the first element of $\mathtt{L}$, while $y$ is the last element of
      $\mathtt{L}$.  Then, the list 
      $\mathtt{L}$ is split into three parts:
      \begin{enumerate}
      \item The first part contains those elements that are less than $x$.
      \item The second part contains those elements that are bigger or equal than $x$ but less or
            equal than $y$.
      \item The third part contains those elements that are bigger than $y$.
      \end{enumerate}
      Figure \ref{fig:dual-pivot-quick-sort.stlx} on page \pageref{fig:dual-pivot-quick-sort.stlx}
      shows a simple list based implementation of \blue{dual pivot quicksort}.



      Various studies have shown that, on average, \blue{dual pivot quicksort} is faster than any other sorting
      algorithm.  For this reason, the version 1.7 of \textsl{Java} uses \blue{dual pivot quicksort}:
      \\[0.2cm]
      \hspace*{0.3cm}
      \href{http://www.docjar.com/html/api/java/util/DualPivotQuicksort.java.html}{http://www.docjar.com/html/api/java/util/DualPivotQuicksort.java.html} 
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    sort := procedure(L) {
        match (L) {
        case []     : return [];
        case [x]    : return [x];
        case [x,y|r]: 
             [p1, p2] := [min({x,y}), max({x,y})];
             [L1,L2,L3] := partition(p1, p2, r);
             return sort(L1) + [p1] + sort(L2) + [p2] + sort(L3);
        }
    };
    partition := procedure(p1, p2, L) {
        match (L) {
        case []   : return [ [], [], [] ];
        case [x|r]: [ r1, r2, r3 ] := partition(p1, p2, r);
                    if (x < p1) {
                        return [ [x|r1], r2, r3 ];
                    } else if (x <= p2) {
                        return [ r1, [x|r2], r3 ];
                    } else {
                        return [ r1, r2, [x|r3] ];
                    }
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A list based implementation of \blue{dual pivot quicksort}.}
\label{fig:dual-pivot-quick-sort.stlx}
\end{figure}

\exercise
Implement a version of \blue{dual pivot quicksort} that uses arrays instead of lists.


\section[A Lower Bound]{A Lower Bound for the Number of Comparisons Needed to Sort a List}
In this section we will show that any sorting algorithm that sorts elements by comparing them, must
use at least 
\\[0.2cm]
\hspace*{1.3cm}
 $\Omega\bigl(n \cdot \log_2(n)\bigr)$ 
\\[0.2cm]
comparisons.  The important caveat here is that the sorting algorithm is not permitted to make any assumptions
on the elements of the list $L$ that is to be sorted.  The only operation that is allowed on these
elements is the use of the comparison operator ``\texttt{<}''.  Furthermore, to simplify matters let
us assume that all elements of the list $L$ are distinct.

Let us consider lists of two elements first, i.~e.~assume we have
\\[0.2cm]
\hspace*{1.3cm}
$L = [a_1, a_2]$.  
\\[0.2cm]
In order to sort this list, one comparison is sufficient:
\begin{enumerate}
 \item If $a_1 < a_2$ then $[a_1, a_2]$ is sorted ascendingly.
 \item If $a_2 < a_1$ then $[a_2, a_1]$ is sorted ascendingly.
\end{enumerate}
If the list $L$ that is to be sorted has the form
\\[0.2cm]
\hspace*{1.3cm}
$L = [a_1,a_2,a_3]$,
\\[0.2cm]
then there are 6 possibilities to arrange these elements:
\\[0.2cm]
\hspace*{0.3cm}
$[a_1,a_2,a_3]$, \quad
$[a_1,a_3,a_2]$, \quad
$[a_2,a_1,a_3]$, \quad
$[a_2,a_3,a_1]$, \quad
$[a_3,a_1,a_2]$, \quad
$[a_3,a_2,a_1]$. 
\\[0.2cm]
Therefore, we need at least three comparisons, since with two comparisons we could at most choose between
four different possibilities.  In general, there are 
\\[0.2cm]
\hspace*{1.3cm}
$\ds n! = 1 \cdot 2 \cdot 3 \cdot {\dots} \cdot (n-1) \cdot n = \prod\limits_{i=1}^n i$ 
\\[0.2cm]
different permutations of a list of $n$ different elements. 
We prove this claim by induction. 
\begin{enumerate}
\item[B.C.:] $n=1$:  

      There is only $1$ way to arrange one element in a list.  As $1 = 1!$ the claim ist proven
      in this case.
\item[I.S.:] $n \mapsto n+1$:
  
      If we have $n+1$ different elements and want to arrange these elements in a list, then there
      are $n+1$ possibilities for the first element.  In each of these cases the induction
      hypothesis tells us that there are $n!$ ways to arrange the remaining $n$ elements in a list.
      Therefore, all in all there are $(n+1) \cdot n! = (n+1)!$ different arrangements of $n+1$
      elements in a list.
\end{enumerate}
Next, we consider how many different cases can be distinguished if we have $k$ different tests
that only give yes or no answers.  Tests of this kind are called \blue{binary tests}.
\begin{enumerate}
\item If we restrict ourselves to binary tests, then one test can only distinguish between two cases.
\item If we have $2$ tests, then we can distinguish between  $2^2$ different cases.
\item In general, $k$ tests can choose from at most $2^k$ different cases.
\end{enumerate}
The last claim can be argued as follows:  If the results of the tests are represented as
$0$ and $1$, then $k$ binary tests correspond to a binary string of length
$k$.  However, binary strings of length $k$ can be used to code the numbers from $0$ up to
$2^{k}-1$.  We have
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{card}\bigl(\{0,1,2,\cdots, 2^k-1\}\bigr) = 2^k$.
\\[0.2cm]
Hence there are $2^k$ binary strings of length $k$.  

If we have a list of $n$ different elements then there are $n!$ different permutations of these
elements.  In order to figure out which of these $n!$ different permutations is given we have to
perform $k$ comparisons where we must have
\\[0.2cm]
\hspace*{1.3cm}
$2^k \geq n!$.
\\[0.2cm]
This immediately implies
\\[0.2cm]
\hspace*{1.3cm}
$k \geq \log_2(n!)$.
\\[0.2cm]
In order to proceed, we need an approximation for the expression $\log_2(n!)$.  
A \href{http://en.wikipedia.org/wiki/Stirling's_approximation}{simple approximation} of this term is
\\[0.2cm]
\hspace*{1.3cm}
$\log_2(n!) = n \cdot \log_2(n) + \Theta(n)$.
\\[0.2cm]
Using this approximation we get
\\[0.2cm]
\hspace*{1.3cm}
$k \geq n \cdot \log_2(n) + \Theta(n)$.
\\[0.2cm]
As \blue{merge sort} is able to sort a list of length $n$ using only $n \cdot \log_2(n)$ comparisons
we have shown that this algorithm is optimal with respect to the number of comparisons.

\section{\href{https://en.wikipedia.org/wiki/Counting_sort}{Counting Sort}}
In the last section of this chapter we introduce a sorting algorithm that has only a linear complexity.  According
to the result of the previous section this algorithm does not work by comparing the elements of the list that
is to be sorted.  Rather, this algorithm exploits the fact that the keys used for sorting are natural
numbers.  We explain counting sort with the help of an example.  Table \ref{tab:marks} on page
\pageref{tab:marks} shows a table showing students and their grades. 
As it stands, the names of the students are ordered alphabetically.  However, the teacher would like to sort
the list of students according to their grades.  Within a group of students that have achieved the same grade,
the students should still be ordered alphabetically.  Table \ref{tab:marks-sorted} on page \pageref{tab:marks-sorted}
shows the table that has been sorted accordingly.

\begin{table}[!ht]
  \centering
  \begin{tabular}{|l|c|}
    \hline
    Student   & Grade \\
    \hline
    \hline
    Alexander & 4 \\
    \hline
    Benjamin  & 2 \\
    \hline
    Daniel    & 3 \\
    \hline
    David     & 3 \\
    \hline
    Elijah    & 2 \\
    \hline
    Gabriel   & 1 \\
    \hline
    Henry     & 2 \\
    \hline
    Jacob     & 5 \\
    \hline
    James     & 3 \\
    \hline
    Joseph    & 2 \\
    \hline
    Liam      & 2 \\
    \hline
    Logan     & 3 \\
    \hline
    Lucas     & 1 \\
    \hline
    Mason     & 2 \\
    \hline
    Matthew   & 5 \\
    \hline
    Michael   & 3 \\
    \hline
    Noah      & 4 \\
    \hline
    Oliver    & 2 \\
    \hline
    Owen      & 4 \\
    \hline
    Samuel    & 3 \\
    \hline
    Sebastian & 2 \\
    \hline
    William   & 1 \\
    \hline
  \end{tabular}
  \caption{Students and their grades, sorted alphabetically.}
  \label{tab:marks}
\end{table}

\begin{table}[!ht]
  \centering
  \begin{tabular}{|l|c|}
    \hline
    Student   & Grade \\
    \hline
    \hline
    Gabriel   & 1 \\
    \hline
    Lucas     & 1 \\
    \hline
    William   & 1 \\
    \hline
    Benjamin  & 2 \\
    \hline
    Elijah    & 2 \\
    \hline
    Henry     & 2 \\
    \hline
    Joseph    & 2 \\
    \hline
    Liam      & 2 \\
    \hline
    Mason     & 2 \\
    \hline
    Oliver    & 2 \\
    \hline
    Sebastian & 2 \\
    \hline
    Daniel    & 3 \\
    \hline
    David     & 3 \\
    \hline
    James     & 3 \\
    \hline
    Logan     & 3 \\
    \hline
    Michael   & 3 \\
    \hline
    Samuel    & 3 \\
    \hline
    Alexander & 4 \\
    \hline
    Noah      & 4 \\
    \hline
    Owen      & 4 \\
    \hline
    Jacob     & 5 \\
    \hline
    Matthew   & 5 \\
    \hline
  \end{tabular}
  \caption{Students and their grades, sorted with respect to the grade.}
  \label{tab:marks-sorted}
\end{table}
We proceed to describe an algorithm that is capable of sorting the Table \ref{tab:marks} into the table
\ref{tab:marks-sorted}.  The algorithm works in three stages.
\begin{enumerate}
\item The first stage is the \blue{counting stage}.  In this stage we count the number of students
      that have a specific grades.  In the example from Table \ref{tab:marks} we find the following:
      \begin{enumerate}[(a)]
      \item 3 students have grade 1.
      \item 8 students have grade 2.
      \item 6 students have grade 3.
      \item 3 students have grade 4.
      \item 2 students have grade 5.
      \end{enumerate}
\item The second stage is the \blue{indexing stage}.   From the previous stage we know that the sorted
      list $\texttt{L}$ that is returned must have the following outline.
      \begin{enumerate}[(a)]
      \item The sublist $\texttt{L[1..3]}$ contains the students with grade 1.
      \item The sublist $\texttt{L[4..11]}$ contains the students with grade 2.
      \item The sublist $\texttt{L[12..17]}$ contains the students with grade 3.
      \item The sublist $\texttt{L[18..20]}$ contains the students with grade 4.
      \item The sublist $\texttt{L[21..22]}$ contains the students with grade 5.
      \end{enumerate}
      The indexing stage computes the \blue{starting indices} of these sublists.
      Of course,  the first sublist has to start at the index $1$.  Since there are 3 students with a grade of 1,
      the second sublist starts at the index $1 + 3 = 4$.  Since there are 8 students with a grade of 2, the third
      sublist starts at the index $1 + 3 + 8 = 12$.  In general, if the sublist for the students with grade $g$
      starts at index $i_g$ and there are $n_g$ students that have achieved the grade $g$, then the sublist for the
      students with grade $g+1$ starts at index $i_{g+1}$ where
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds i_{g+1} = i_g + n_g$.
\item The \blue{distribution stage} iterates over the list of students and inserts them into the sublists
      corresponding to the grades of the students.
\end{enumerate}
Figure \ref{fig:counting-sort.stlx} on page \pageref{fig:counting-sort.stlx} shows an implementation of
counting sort.  We proceed to discuss this algorithm line by line.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    countingSort := procedure(Names, Grades) {
        assert(#Names == #Grades, "$#Names$ != $#Grades$");
        maxGrade     := max(Grades);
        Counts       := [0] * maxGrade;
        Indices      := [];
        SortedNames  := [];
        SortedGrades := [];
        for (g in Grades) {
            Counts[g] += 1;
        }
        Indices[1] := 1;
        for (g in [2 .. maxGrade]) {
            Indices[g] := Indices[g-1] + Counts[g-1];
        }
        for (i in [1 .. #Names]) {
            g     := Grades[i];
            index := Indices[g];
            SortedNames [index] := Names[i];
            SortedGrades[index] := Grades[i];
            Indices[g]          += 1;
        }
        return [SortedNames, SortedGrades];
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{An Implementation of \blue{counting sort}.}
\label{fig:counting-sort.stlx}
\end{figure}

\begin{enumerate}
\item The procedure \texttt{countingSort} receives two parameters:
      \begin{enumerate}
      \item \texttt{Names} is a list of the student names that is sorted alphabetically.
      \item \texttt{Grades} is a list of the grades attached to the students.
      \end{enumerate}
      Of course, these lists are required to have the same number of elements.  This is checked in the
      \texttt{assert} statement in line 2.
\item In order for our routine to generalize to arbitrary grades, we need to compute the maximum of all grades
      and store it the variable \texttt{maxGrade}.  Of course, in the example discussed so far we know that the biggest
      grade is $5$.  However, in general the grades can be any numbers and the procedure \texttt{countingSort}
      is able to sort the \texttt{Names} according to these grades.  
\item Next, we initialize the auxiliary array \texttt{Count} to be an array of length \texttt{maxGrade}.
      Later, for a grade $g$ the number $\texttt{Counts}[g]$ will contain the number of students that have
      attained the grade $g$.  Initially, all entries of the array \texttt{Count} are set to $0$.
\item After the indexing stage, the array \texttt{Indices} will contain the start indices of the different sublists.
      For a grade $g$, $\texttt{Indices}[g]$ is the first index of the sublist containing those students that
      have achieved the grade $g$.
\item The array \texttt{SortedNames} and \texttt{SortedGrades} are the two lists that will be returned as the
      result.  \texttt{SortedNames} will contain the names of the students sorted by their grades, while
      \texttt{SortedGrades} will contain the corresponding grades. 
\item The \texttt{for}-loop in line 8 performs the \blue{counting stage}.  We iterate over all grades $g$ in
      \texttt{Grades} and increment the counter $\texttt{Counts}[g]$ associated with the grade $g$.
\item Next, the index for the start of the sublist containing those students that have achieved the grade $1$
      is initialized as $1$ in line 11. 
\item Then, the \texttt{for}-loop in line 12 performs the \blue{indexing stage}. As the number $\texttt{Indices}[g-1]$
      is the index of the start of the sublist for those students that have grade $g-1$ and the number of these
      students is $\texttt{Counts}[g-1]$, the sublist of the students with grade $g$ has to start at 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{Indices}[g-1] + \texttt{Counts}[g-1]$.
\item Finally, the \texttt{for}-loop in line 15 performs the \blue{distribution stage}.
      \begin{enumerate}
      \item Conceptually, the \texttt{for}-loop iterates over all students. The index $i$ refers to the $i$-th
            student in the original list.
      \item Next, we need to find where to put the $i$-th student.  To this end we first look up the
            grade $g = \texttt{Grades}[i]$ of this student.  Then, $\texttt{Indices}[g]$ gives us
            the \texttt{index} of the next free entry in the result list \texttt{SortedNames} corresponding to grade $g$.
      \item In line 17 and 18 the student and his grade are stored in the result lists \texttt{SortedNames}
            and \texttt{SortedGrades} at the computed \texttt{index}.
      \item Finally, we need to increment the index stored at $\texttt{Indices}[g]$ since we have just used
            this index and therefore the next student with the grade $g$ needs to be stored at the subsequent location.
            This is done in line 20.
      \end{enumerate}
\item The procedure \texttt{countingSort} returns a pair of lists.
      \begin{enumerate}
      \item The first list is the list of students sorted with respect to their grades.
      \item The second list is the list of the corresponding grades.
      \end{enumerate}
\end{enumerate}
If the list \texttt{Names} has a length of $n$ and the biggest grade is some fixed constant $c$, then it is easy to
see that counting sort has the complexity $\Oh(n)$.  The reason is that the first \texttt{for}-loop is iterated
$n$ times, the second \texttt{for}-loop is iterated just $c$ times and the lust \texttt{for}-loop is again
iterated $n$ times.  Hence, counting sort is a linear sorting algorithm.  Note that we do not use any
comparisons in this algorithm.

Another important fact is that counting sort is \blue{stable}: In the resulting list, the sublists
corresponding to the different grades are still sorted alphabetically.  This is so because these sublists are
filled by iterating over the original list that is sorted alphabetically.  If two students $x$ and $y$ have the
same grade $g$ but the name of $x$ is alphabetically before the name of $y$, then $x$ will be inserted into the
sublist corresponding grade $g$ before $y$ and hence these sublists are ordered alphabetically.  This property
is crucial for the development of our next sorting algorithm \blue{radix sort}.

\section{\href{https://en.wikipedia.org/wiki/Radix_sort}{Radix Sort}}
The importance of the previous sorting algorithm, counting sort, stems from the fact that it is part of the
implementation of \blue{radix sort}.  Radix sort was used as early as 1887 in
\href{https://en.wikipedia.org/wiki/Tabulating_machine}{tabulating machines} constructed by
\href{https://en.wikipedia.org/wiki/Herman_Hollerith}{Hermann Hollerith}.  He was the founder of
the \blue{Tabulating Machine Company} that later became \href{https://en.wikipedia.org/wiki/IBM}{\textsc{Ibm}}.
To understand radix sort, suppose we want to implement an algorithm that sorts a large number of 32 bit
unsigned integers.  The easiest way to do this would be to use counting sort:  In that case the names of the
students would be unimportant and could be filled with an arbitrary string.  However, since the grades could then
have a maximum value of 
\\[0.2cm]
\hspace*{1.3cm}
$2^{32} - 1 = 4,294,967,295$,
\\[0.2cm]
the lists \texttt{Counts} and \texttt{Indices} that we have used in the function \texttt{countingSort} would
become huge and would use 16 gigabyte.   This size is prohibitive.  The main idea of radix sort is to split the
32 bit numbers into four chunks of 8 bits each and to use each of these four chunks as a grade.  To formulate this
mathematically, a 32 bit unsigned integer $x$ is decomposed into its four bytes as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds x = b_4 \cdot 256^{3} + b_3 \cdot 256^{2} + b_2 \cdot 256^{1} + b_1$.
\\[0.2cm]
Note that have  numbered the bytes starting from the the least significant byte.
Then, radix sort works as follows:
\begin{enumerate}
\item Sort the numbers by interpreting the byte $b_1$ as a grade.
\item Take the numbers that have been sorted by the byte $b_1$ and sort them by the byte $b_2$ next.
      Since counting sort is \blue{stable}, numbers which happen to have the same byte $b_2$ will still be
      sorted with respect to by the byte $b_1$.  Hence, after the sorting with respect to the byte $b_2$ is
      complete, in effect the numbers will then be sorted according to both $b_2$ and $b_1$.
\item Next, use counting sort to sort the numbers with respect to the byte $b_3$.
\item Finally, use counting sort to sort the numbers with respect to the byte $b_4$.  By the stability of
      counting sort, the numbers are now sorted with respect to all of their byte, where $b_4$ is the most
      significant byte and $b_1$ is the least significant byte. Hence, the numbers are sorted. 
\end{enumerate}


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                  ]
    load("counting-sort.stlx");
    
    extractByte := [x, k] |-> (x \ 256 ** (k-1)) % 256; 
    
    radixSort := procedure(L) {
        for (k in [1..4]) {
            Grades := [extractByte(x, k) + 1: x in L];
            L := countingSort(L, Grades)[1];
        }
        return L;
    };                  
\end{Verbatim}
\vspace*{-0.3cm}
\caption{An implementation of \blue{radix sort} for sorting unsigned 32 integers.}
\label{fig:radix-sort.stlx}
\end{figure}
Figure \ref{fig:radix-sort.stlx} on page \pageref{fig:radix-sort.stlx} shows an implementation of radix
sort that implements these ideas.
\begin{enumerate}
\item The function \texttt{extractByte} is called with two arguments:
      \begin{enumerate}[(a)]
      \item $x$ is supposed to be an unsigned 32 bit number.  With respect to the programming language
            \textsc{SetlX} this just means that $x$ is a integer satisfying $0 \leq x < 2^{32}$.
      \item $k$ is the index of the byte that is to be extracted.  It is supposed that the least significant
            byte has the index $1$.
      \end{enumerate}
      Hence, $\texttt{extractByte}(x, k)$ extracts the $k$-th byte of the number $x$.

      \texttt{extractByte} works by shifting the number $x$ by $(k-1) \cdot 8$ bits to the left using an
      integer division.  Then, the least significant byte of the resulting number is extracted using the modulus
      operator. 
\item \texttt{radixSort} takes a list of unsigned 32 bit integers $L$ as its arguments.
\item It iterates over the four bytes of these numbers starting with the least significant byte.
\item In order to be able to interpret a byte as a grade we have to add 1 to it, since counting sort
      assumes that the lowest grade is $1$.  The reason is that grades are used as array indices and in
      \textsc{SetlX} list indices start at index $1$.
\item With respect to counting sort,  the elements of the list given to counting sort as its first argument are
      just arbitrary objects.  Therefore, it does not matter whether the first argument to the function
      \texttt{countingSort} is a list of strings interpreted as student names or a list of numbers.  This list
      is sorted with respect to the second argument, the nature of the elements of this list is
      irrelevant.  Therefore, in line 8 the list $L$ is sorted with respect to the $k$-th byte. 
\item When $L$ is returned, this list is sorted with respect to all of the four bytes making up its numbers and
      hence, it is sorted.
\end{enumerate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "algorithms"
%%% End: 
